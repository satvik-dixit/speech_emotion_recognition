{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyDk4cxk1wh"
      },
      "source": [
        "# Speech Emotion Recognition Demo\n",
        "\n",
        "A demo containing all steps of speech emotion recoginition using the EmoDB dataset. The demo has been divided into 3 phases:\n",
        "- Phase 1: Loading audio files and extracting metadata\n",
        "- Phase 2: Embedding Extraction\n",
        "- Phase 3: Downstream Task - Speech Emotion Recognotion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About EmoDB:\n",
        "- A German database of emotional speech\n",
        "- 800 recordings\n",
        "- 10 actors (5 males and 5 females)\n",
        "- 7 emotions: anger, neutral, fear, boredom, happiness, sadness, disgust\n",
        "\n",
        "### References:\n",
        "- Dataset: http://emodb.bilderbar.info/index-1280.html\n",
        "- Paper: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8506&rep=rep1&type=pdf\n",
        "\n",
        "Lets start by importing a few packages.\n"
      ],
      "metadata": {
        "id": "pZ4u3Q5X47hN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48qMjEB-CLk"
      },
      "source": [
        "### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hed_5ILyX0ia",
        "outputId": "aecd7073-c9b4-4338-9cf4-82bd3aec7c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 496 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 57.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 109 kB 57.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 546 kB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 38.0 MB/s \n",
            "\u001b[?25h  Building wheel for hyperpyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.7 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 635 kB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 167 kB 56.2 MB/s \n",
            "\u001b[?25h  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q speechbrain\n",
        "!pip install -q  transformers\n",
        "!git clone -q https://github.com/GasserElbanna/serab-byols.git\n",
        "!python3 -m pip install -q -e ./serab-byols\n",
        "\n",
        "!pip install -q tqdm==4.60.0\n",
        "!pip install -q opensmile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTY1RZ4UwCsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from random import sample\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import opensmile\n",
        "import serab_byols\n",
        "from transformers import Wav2Vec2Model, HubertModel\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "2r74_jiSWxp9",
        "outputId": "9e58ac93-37a1-4c15-ee1a-307a5221fc4c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65c0a496-e11c-4394-bdb5-6e5617fa3941\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65c0a496-e11c-4394-bdb5-6e5617fa3941\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving utilities.py to utilities.py\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Name directory\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6Yipn9MTLg"
      },
      "source": [
        "# Phase 1: Loading audio files and extracting metadata\n",
        "\n",
        "Includes downloading the dataset, loading audio files, resampling audio files, extracting metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a function for loading and resampling audio files"
      ],
      "metadata": {
        "id": "mcSr1Af5YzIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae5OWFUETeZC"
      },
      "outputs": [],
      "source": [
        "# Defining a function for loading and resampling audio files\n",
        "\n",
        "def load_audio_files(audio_files, resampling_frequency=16000, audio_list=None):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_files: string\n",
        "      The paths of the wav files \n",
        "  resampling_frequency: integer\n",
        "      The frequency which all audios will be resampled to\n",
        "  audio_list: list \n",
        "      The list of torch tensors of audios to which more audios need too be added, empty by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of torch tensors, one array for each audio file\n",
        "\n",
        "  '''\n",
        "  # Making audio_list\n",
        "  if audio_list is None:\n",
        "    audio_list = []\n",
        "\n",
        "  # Resampling\n",
        "  for audio in audio_files:\n",
        "    signal, fs = librosa.load(audio, sr=resampling_frequency)\n",
        "    audio_list.append(torch.from_numpy(signal))\n",
        "      \n",
        "  return audio_list\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and resampling audiofiles and collecting metadata on EmoDB dataset"
      ],
      "metadata": {
        "id": "PtuDXb9-OSZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Jncl071Q_z",
        "outputId": "91d41ad7-a3d9-4e4d-bb41-7b0d887ded2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files: 535\n",
            "Number of speaker classes: 10\n",
            "Speaker classes: {'15', '03', '14', '12', '16', '09', '11', '13', '10', '08'}\n",
            "Number of speakers: 535\n",
            "Speakers:\n",
            "['03' '14' '09' '15' '13' '13' '09' '16' '16' '13' '12' '08' '03' '15'\n",
            " '16' '08' '13' '15' '14' '12' '09' '11' '14' '09' '14' '11' '15' '11'\n",
            " '13' '10' '13' '10' '15' '14' '12' '15' '11' '14' '08' '15' '12' '08'\n",
            " '11' '13' '14' '03' '16' '13' '16' '16' '09' '16' '12' '14' '12' '15'\n",
            " '08' '14' '03' '08' '14' '09' '14' '12' '13' '13' '10' '03' '16' '14'\n",
            " '14' '08' '11' '10' '08' '14' '15' '15' '10' '09' '09' '03' '13' '14'\n",
            " '03' '03' '10' '16' '10' '16' '03' '11' '11' '08' '10' '11' '14' '08'\n",
            " '16' '10' '14' '03' '12' '14' '08' '10' '11' '11' '03' '14' '11' '16'\n",
            " '10' '13' '15' '13' '11' '12' '11' '09' '16' '14' '16' '16' '11' '14'\n",
            " '11' '16' '03' '14' '15' '15' '14' '03' '13' '09' '12' '03' '11' '12'\n",
            " '13' '10' '10' '08' '14' '12' '08' '09' '08' '09' '13' '09' '03' '11'\n",
            " '16' '03' '09' '13' '12' '11' '10' '09' '11' '08' '15' '16' '15' '11'\n",
            " '15' '03' '15' '08' '16' '13' '15' '16' '16' '11' '16' '09' '03' '14'\n",
            " '15' '08' '14' '14' '10' '09' '08' '14' '13' '09' '09' '16' '14' '10'\n",
            " '15' '11' '03' '13' '16' '13' '16' '13' '14' '12' '12' '08' '13' '15'\n",
            " '13' '09' '15' '12' '09' '11' '15' '14' '14' '12' '11' '08' '10' '16'\n",
            " '15' '13' '09' '16' '16' '16' '15' '03' '08' '11' '16' '14' '16' '12'\n",
            " '16' '11' '11' '16' '03' '03' '11' '16' '16' '14' '10' '14' '16' '15'\n",
            " '08' '16' '13' '14' '03' '14' '09' '15' '03' '09' '14' '11' '11' '11'\n",
            " '16' '08' '13' '16' '12' '13' '08' '15' '11' '10' '08' '16' '03' '03'\n",
            " '10' '08' '15' '08' '03' '15' '15' '14' '10' '10' '16' '03' '14' '03'\n",
            " '09' '14' '03' '08' '15' '10' '11' '14' '14' '13' '08' '08' '14' '16'\n",
            " '09' '11' '11' '15' '16' '14' '11' '11' '11' '03' '10' '16' '10' '15'\n",
            " '15' '14' '15' '09' '14' '14' '14' '11' '12' '16' '13' '16' '13' '10'\n",
            " '03' '03' '15' '03' '13' '03' '08' '09' '12' '14' '10' '16' '09' '12'\n",
            " '14' '15' '14' '14' '13' '12' '13' '12' '10' '11' '11' '13' '12' '10'\n",
            " '10' '08' '16' '08' '16' '13' '08' '09' '16' '13' '09' '09' '08' '14'\n",
            " '16' '11' '14' '13' '16' '13' '11' '15' '12' '16' '12' '16' '13' '15'\n",
            " '03' '08' '14' '14' '13' '15' '03' '09' '16' '03' '12' '13' '03' '13'\n",
            " '11' '13' '08' '03' '12' '14' '16' '09' '13' '08' '08' '08' '09' '15'\n",
            " '14' '08' '12' '15' '16' '15' '14' '09' '08' '16' '11' '16' '09' '13'\n",
            " '15' '12' '14' '13' '14' '13' '10' '15' '11' '15' '15' '11' '11' '10'\n",
            " '03' '11' '16' '03' '11' '14' '13' '14' '03' '03' '16' '13' '15' '16'\n",
            " '13' '10' '12' '16' '10' '03' '10' '16' '15' '12' '12' '08' '14' '08'\n",
            " '16' '11' '08' '14' '10' '10' '08' '09' '08' '14' '16' '08' '13' '16'\n",
            " '08' '13' '10' '14' '11' '03' '13' '13' '09' '11' '08' '03' '03' '15'\n",
            " '12' '09' '14' '16' '16' '16' '03' '08' '08' '13' '08' '15' '11' '13'\n",
            " '15' '09' '09' '08' '13' '08' '15' '08' '15' '16' '08' '13' '08' '15'\n",
            " '13' '13' '09']\n",
            "Number of label classes: 7\n",
            "Label classes: {'T', 'N', 'F', 'L', 'A', 'E', 'W'}\n",
            "Number of labels: 535\n",
            "Labels:\n",
            "['T' 'T' 'W' 'L' 'A' 'T' 'N' 'E' 'E' 'T' 'A' 'A' 'N' 'F' 'W' 'W' 'E' 'L'\n",
            " 'L' 'W' 'T' 'E' 'A' 'N' 'F' 'E' 'T' 'L' 'T' 'L' 'L' 'A' 'L' 'W' 'W' 'W'\n",
            " 'W' 'N' 'N' 'N' 'E' 'L' 'A' 'T' 'W' 'W' 'W' 'A' 'W' 'L' 'W' 'T' 'A' 'L'\n",
            " 'W' 'F' 'F' 'F' 'W' 'L' 'W' 'N' 'E' 'W' 'T' 'E' 'L' 'L' 'L' 'E' 'T' 'L'\n",
            " 'A' 'W' 'W' 'E' 'A' 'W' 'W' 'W' 'E' 'L' 'L' 'W' 'F' 'T' 'L' 'W' 'T' 'E'\n",
            " 'L' 'N' 'A' 'W' 'N' 'N' 'E' 'N' 'A' 'T' 'L' 'N' 'W' 'W' 'W' 'F' 'A' 'T'\n",
            " 'W' 'E' 'A' 'W' 'A' 'F' 'A' 'W' 'F' 'W' 'N' 'N' 'W' 'A' 'T' 'F' 'F' 'W'\n",
            " 'F' 'F' 'N' 'A' 'W' 'A' 'W' 'W' 'N' 'W' 'T' 'W' 'N' 'N' 'F' 'A' 'W' 'F'\n",
            " 'A' 'L' 'N' 'W' 'W' 'E' 'A' 'L' 'L' 'W' 'T' 'T' 'L' 'L' 'W' 'T' 'N' 'E'\n",
            " 'T' 'W' 'N' 'E' 'A' 'N' 'N' 'N' 'W' 'A' 'L' 'W' 'N' 'A' 'T' 'L' 'E' 'T'\n",
            " 'W' 'N' 'N' 'L' 'A' 'E' 'E' 'F' 'T' 'L' 'A' 'E' 'L' 'L' 'T' 'F' 'L' 'W'\n",
            " 'T' 'L' 'W' 'L' 'L' 'W' 'E' 'L' 'A' 'A' 'N' 'N' 'N' 'F' 'T' 'T' 'T' 'L'\n",
            " 'L' 'L' 'T' 'L' 'F' 'A' 'A' 'F' 'W' 'L' 'T' 'F' 'N' 'E' 'A' 'W' 'A' 'L'\n",
            " 'F' 'W' 'T' 'W' 'W' 'W' 'T' 'L' 'T' 'F' 'N' 'E' 'A' 'N' 'L' 'N' 'W' 'A'\n",
            " 'F' 'F' 'F' 'N' 'N' 'W' 'W' 'F' 'A' 'W' 'F' 'W' 'L' 'N' 'N' 'N' 'F' 'L'\n",
            " 'A' 'W' 'F' 'F' 'A' 'W' 'L' 'T' 'W' 'W' 'N' 'F' 'E' 'F' 'F' 'L' 'F' 'T'\n",
            " 'A' 'W' 'L' 'A' 'W' 'A' 'W' 'W' 'N' 'T' 'W' 'T' 'N' 'W' 'W' 'E' 'N' 'L'\n",
            " 'L' 'E' 'N' 'W' 'A' 'T' 'L' 'F' 'L' 'F' 'A' 'T' 'A' 'W' 'L' 'A' 'L' 'N'\n",
            " 'W' 'W' 'A' 'A' 'F' 'F' 'F' 'N' 'A' 'L' 'E' 'L' 'E' 'F' 'W' 'W' 'L' 'W'\n",
            " 'W' 'N' 'T' 'A' 'N' 'N' 'N' 'A' 'E' 'E' 'W' 'T' 'A' 'L' 'F' 'E' 'W' 'T'\n",
            " 'A' 'N' 'N' 'F' 'L' 'T' 'A' 'W' 'A' 'W' 'L' 'W' 'W' 'W' 'L' 'W' 'A' 'A'\n",
            " 'F' 'L' 'F' 'F' 'T' 'N' 'W' 'E' 'L' 'W' 'W' 'F' 'W' 'L' 'F' 'N' 'L' 'L'\n",
            " 'L' 'W' 'W' 'N' 'T' 'N' 'N' 'N' 'N' 'W' 'N' 'F' 'T' 'T' 'F' 'W' 'W' 'E'\n",
            " 'N' 'W' 'T' 'L' 'W' 'N' 'T' 'T' 'W' 'W' 'L' 'A' 'A' 'W' 'N' 'F' 'W' 'A'\n",
            " 'E' 'L' 'E' 'W' 'T' 'F' 'A' 'L' 'W' 'N' 'W' 'W' 'F' 'A' 'F' 'A' 'W' 'W'\n",
            " 'L' 'A' 'T' 'A' 'W' 'F' 'L' 'N' 'F' 'F' 'W' 'T' 'A' 'A' 'W' 'E' 'W' 'F'\n",
            " 'L' 'F' 'N' 'N' 'T' 'L' 'W' 'N' 'L' 'W' 'F' 'F' 'W' 'F' 'N' 'N' 'N' 'T'\n",
            " 'E' 'W' 'E' 'W' 'F' 'W' 'W' 'N' 'F' 'N' 'E' 'F' 'E' 'T' 'F' 'W' 'N' 'W'\n",
            " 'A' 'F' 'T' 'E' 'L' 'N' 'F' 'L' 'T' 'W' 'W' 'T' 'L' 'W' 'L' 'A' 'E' 'T'\n",
            " 'E' 'T' 'N' 'F' 'N' 'A' 'W' 'E' 'F' 'E' 'N' 'N' 'F']\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -q -d piyushagni5/berlin-database-of-emotional-speech-emodb\n",
        "! unzip -q berlin-database-of-emotional-speech-emodb.zip\n",
        "\n",
        "# Resample dataset\n",
        "audio_files = glob('/content/wav/*.wav')\n",
        "audio_list = load_audio_files(audio_files, resampling_frequency=16000)\n",
        "\n",
        "# Extracting metadata (speakers and labels)\n",
        "labels = np.array(list(map(lambda x: os.path.basename(x).split('.')[0][-2], audio_files)))\n",
        "speakers = np.array(list(map(lambda x: os.path.basename(x)[:2], audio_files)))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Speakers:')\n",
        "print(speakers)\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))\n",
        "print('Labels:')\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kPY4usMGbA"
      },
      "source": [
        "# Phase 2: Embedding Extraction\n",
        "Includes extracting features using\n",
        "- Deep learning based methods: Hybrid BYOL-S\n",
        "- DSP based methods: openSMILE compare, openSMILE egemaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3vG54u9D-r"
      },
      "source": [
        "### Audio embeddings extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCNer33gwaMl"
      },
      "outputs": [],
      "source": [
        "# Defining a function for generating audio embedding extraction models\n",
        "\n",
        "def audio_embeddings_model(model_name):\n",
        "  '''\n",
        "  Generates model for embedding extraction \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  mode_name: string\n",
        "      The model to used, could be 'hybrid_byols', 'compare' or 'egemaps'\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  model: object\n",
        "      The embedding extraction model\n",
        "  '''\n",
        "  if model_name=='hybrid_byols':\n",
        "    model_name = 'cvt'\n",
        "    checkpoint_path = \"serab-byols/checkpoints/cvt_s1-d1-e64_s2-d1-e256_s3-d1-e512_BYOLAs64x96-osandbyolaloss6373-e100-bs256-lr0003-rs42.pth\"\n",
        "    model = serab_byols.load_model(checkpoint_path, model_name)\n",
        "  elif model_name=='compare':\n",
        "    model = opensmile.Smile(\n",
        "        feature_set=opensmile.FeatureSet.ComParE_2016,\n",
        "        feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    )\n",
        "  elif model_name=='egemaps':\n",
        "    model = opensmile.Smile(\n",
        "        feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "        feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    )\n",
        "  return model\n",
        "\n",
        "\n",
        "# Defining a function for embedding exctraction from the audio list\n",
        "\n",
        "def audio_embeddings(audio_list, model_name, model, sampling_rate=16000):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of arrays, one array for each audio file\n",
        "  model_name: string\n",
        "      The model to used, could be 'hybrid_byols', 'compare' or 'egemaps'\n",
        "  model: object\n",
        "      The embedding extraction model generated by audio_embeddings_model function\n",
        "  sampling_rate: int\n",
        "      The sampling rate, 16 kHz by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  if model_name=='hybrid_byols':\n",
        "    embeddings_array = serab_byols.get_scene_embeddings(audio_list, model)\n",
        "  else:\n",
        "    embeddings_list = []\n",
        "    for i in tqdm(range(len(audio_list))):\n",
        "      embeddings = model.process_signal(audio_list[i], sampling_rate)\n",
        "      embeddings_list.append(torch.tensor(embeddings.values[0], dtype=torch.float32))\n",
        "    embeddings_array = torch.stack(embeddings_list)\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio embeddings extraction on EmoDB"
      ],
      "metadata": {
        "id": "Z__xn5XdPt5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvmZ0RwTDXCY",
        "outputId": "1832d3ac-67f7-4adc-dd7d-bff3ac2cf658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings...: 100%|██████████| 535/535 [00:37<00:00, 14.10it/s]\n",
            "100%|██████████| 535/535 [00:57<00:00,  9.38it/s]\n",
            "100%|██████████| 535/535 [01:02<00:00,  8.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 2048])\n",
            "The embeddings array is: \n",
            "tensor([[ 3.3003,  5.1818,  0.9551,  ...,  5.0923, -1.7271,  4.4259],\n",
            "        [ 3.7851,  4.7061,  1.2274,  ...,  4.3181, -0.3392,  3.7923],\n",
            "        [ 4.6339,  4.9679,  1.4998,  ...,  4.3758,  0.0359,  3.8127],\n",
            "        ...,\n",
            "        [ 3.4200,  4.8069,  0.5398,  ...,  4.8588,  0.4056,  4.0666],\n",
            "        [ 5.8241,  5.3683,  1.2110,  ...,  5.1313,  1.4869,  3.6378],\n",
            "        [ 6.2243,  4.5836,  1.7008,  ...,  4.7435, -0.9581,  3.1392]])\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 6373])\n",
            "The embeddings array is: \n",
            "tensor([[2.7397e+00, 1.9208e-01, 4.5941e-01,  ..., 5.4328e+01, 1.0450e+02,\n",
            "         5.5951e+01],\n",
            "        [2.6716e+00, 2.0376e-01, 5.9561e-01,  ..., 4.5818e+01, 1.0355e+02,\n",
            "         4.9328e+01],\n",
            "        [3.6595e+00, 6.7089e-01, 1.2658e-02,  ..., 6.3352e+01, 1.4530e+02,\n",
            "         6.7398e+01],\n",
            "        ...,\n",
            "        [2.9835e+00, 6.8056e-01, 2.2917e-01,  ..., 6.1027e+01, 1.4902e+02,\n",
            "         5.3635e+01],\n",
            "        [2.9678e+00, 8.7838e-02, 7.1622e-01,  ..., 3.9188e+01, 1.1787e+02,\n",
            "         4.7169e+01],\n",
            "        [3.2487e+00, 1.9858e-01, 1.4184e-02,  ..., 8.8837e+01, 1.5061e+02,\n",
            "         8.3429e+01]])\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 88])\n",
            "The embeddings array is: \n",
            "tensor([[ 2.4418e+01,  2.4826e-01,  2.1172e+01,  ...,  2.3462e-01,\n",
            "          3.6649e-01, -1.9927e+01],\n",
            "        [ 3.2452e+01,  2.8308e-01,  2.8491e+01,  ...,  2.4625e-01,\n",
            "          2.7390e-01, -1.6955e+01],\n",
            "        [ 3.7857e+01,  9.9005e-02,  3.5355e+01,  ...,  1.1400e-01,\n",
            "          6.9455e-02, -2.2807e+01],\n",
            "        ...,\n",
            "        [ 3.6730e+01,  2.0717e-01,  3.3262e+01,  ...,  7.0000e-02,\n",
            "          3.5456e-02, -1.4477e+01],\n",
            "        [ 3.3259e+01,  1.2200e-01,  2.9032e+01,  ...,  8.1667e-02,\n",
            "          3.8042e-02, -1.5619e+01],\n",
            "        [ 4.3111e+01,  8.3356e-02,  4.0573e+01,  ...,  4.0000e-02,\n",
            "          1.8708e-02, -2.0967e+01]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Phase_2\n",
        "\n",
        "# Hybrid BYOLS\n",
        "model = audio_embeddings_model(model_name='hybrid_byols')\n",
        "embeddings_array_byols = audio_embeddings(audio_list, model_name='hybrid_byols', model=model)\n",
        "\n",
        "# EmoDB compare\n",
        "model = audio_embeddings_model(model_name='compare')\n",
        "embeddings_array_compare = audio_embeddings(audio_list, model_name='compare', model=model)\n",
        "\n",
        "# EmoDB egemaps\n",
        "model = audio_embeddings_model(model_name='egemaps')\n",
        "embeddings_array_egemaps = audio_embeddings(audio_list, model_name='egemaps', model=model)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Verify Phase_2\n",
        "models = ['hybrid_byols', 'compare', 'egemaps']\n",
        "embeddings_arrays = {'hybrid_byols': embeddings_array_byols, 'compare':embeddings_array_compare, 'egemaps':embeddings_array_egemaps}\n",
        "\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of the embeddings array is {}'.format(embeddings_arrays[model].shape))\n",
        "  print('The embeddings array is: ')\n",
        "  print((embeddings_arrays[model]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIMNm01HGyZ"
      },
      "source": [
        "# Phase 3: Downstream Task - Speech Emotion Recognotion\n",
        "Includes speaker normalisation, train test splitting and hyperparameter tuning using logistic regression, SVM and random forest classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91OM9gcMHWLN"
      },
      "source": [
        "### Speaker normalisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlFL7hMDwiqk"
      },
      "outputs": [],
      "source": [
        "# Defining a function for speaker normalisation using standard scaler\n",
        "\n",
        "def speaker_normalisation(embeddings_array, speakers):\n",
        "  '''\n",
        "  Normalises embeddings_array for each speaker\n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array of embeddings, one row for each audio file\n",
        "  speakers: list \n",
        "      The list of speakers\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg normalised embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  speaker_ids = set(speakers)\n",
        "  for speaker_id in speaker_ids:\n",
        "    speaker_embeddings_indices = np.where(np.array(speakers)==speaker_id)[0]\n",
        "    speaker_embeddings = embeddings_array[speaker_embeddings_indices,:]\n",
        "    scaler = StandardScaler()\n",
        "    normalised_speaker_embeddings = scaler.fit_transform(speaker_embeddings)\n",
        "    embeddings_array[speaker_embeddings_indices] = torch.tensor(normalised_speaker_embeddings).float()\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speaker normalisation on EmoDB"
      ],
      "metadata": {
        "id": "cS14ei0RSJb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uibu-ZnCDg_D",
        "outputId": "0f00364e-0673-4e48-ee24-27fbfe206a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 2048])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[-1.4416,  1.1102,  0.5387,  ...,  0.0447, -1.5613,  1.9950],\n",
            "        [-1.2288,  0.2675, -0.2237,  ..., -0.2757,  0.2524,  0.9853],\n",
            "        [-0.3926,  0.0199,  0.1643,  ..., -0.7147,  0.0252,  0.5424],\n",
            "        ...,\n",
            "        [-1.6562, -0.2163, -1.9813,  ..., -0.1907,  0.7741,  0.9494],\n",
            "        [ 0.2894,  0.1747, -0.6020,  ...,  0.2717,  1.8460, -0.1170],\n",
            "        [ 1.2788, -0.2910,  0.5713,  ..., -0.1577, -0.8340, -0.7457]])\n",
            "Columnwise_mean:\n",
            "tensor([-3.5651e-09,  8.9128e-10,  0.0000e+00,  ..., -2.6739e-09,\n",
            "        -2.8967e-09,  3.5651e-09])\n",
            "PASSED: All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 6373])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[-0.8150, -0.7810,  0.5549,  ..., -0.8784, -0.8868, -0.6129],\n",
            "        [-0.7592, -0.5905,  0.4798,  ..., -1.7397, -1.2616, -0.9612],\n",
            "        [ 0.7708,  0.7373, -0.9606,  ..., -0.4305,  0.3503, -0.1889],\n",
            "        ...,\n",
            "        [ 0.2059,  1.0500, -0.3189,  ..., -0.4680,  0.5485, -0.9796],\n",
            "        [ 0.1763, -0.9552,  0.8755,  ..., -1.8168, -0.6808, -1.3746],\n",
            "        [-0.1028, -0.8975, -0.9569,  ...,  1.0229,  0.5210,  0.6728]])\n",
            "Columnwise_mean:\n",
            "tensor([-3.1195e-09,  4.9021e-09,  3.5651e-09,  ...,  7.1303e-09,\n",
            "         1.7826e-09,  5.3477e-09])\n",
            "PASSED: All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 88])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[-1.0602,  2.2180, -1.1086,  ...,  2.2235,  3.6607, -0.9170],\n",
            "        [-0.6374,  2.1925, -0.6961,  ...,  2.0614,  2.2765,  0.8543],\n",
            "        [ 0.3475,  0.0195,  0.3058,  ...,  0.3335,  0.3163, -1.8189],\n",
            "        ...,\n",
            "        [ 0.0612,  1.8113, -0.1287,  ..., -0.7441, -0.5382,  1.1298],\n",
            "        [-0.7048,  0.2645, -1.0240,  ..., -0.4965, -0.4997,  0.5566],\n",
            "        [ 1.4514, -0.3461,  1.4473,  ..., -1.3954, -0.9849, -1.0952]])\n",
            "Columnwise_mean:\n",
            "tensor([-1.7826e-09, -3.5651e-09,  4.0108e-09,  9.3585e-09, -1.7826e-09,\n",
            "        -3.1195e-09,  1.7826e-09,  5.3477e-09,  5.3477e-09,  0.0000e+00,\n",
            "         0.0000e+00,  5.3477e-09,  1.7826e-09,  2.2282e-09, -5.3477e-09,\n",
            "         3.5651e-09, -3.5651e-09,  0.0000e+00, -1.7826e-09,  7.1303e-09,\n",
            "        -5.3477e-09,  5.5705e-09, -1.7826e-09,  4.4564e-09, -1.5597e-09,\n",
            "        -7.2417e-09, -1.5597e-09,  6.2390e-09,  2.6739e-09, -6.7960e-09,\n",
            "        -4.4564e-10,  6.6846e-09,  1.7826e-09,  9.9155e-09,  8.9128e-10,\n",
            "        -2.6739e-09, -2.2282e-10, -1.7826e-09,  1.7826e-09, -8.9128e-09,\n",
            "        -1.7826e-09, -4.4564e-10,  4.4564e-09, -3.5651e-09,  3.5651e-09,\n",
            "         1.3369e-09,  1.7826e-09, -1.7826e-09,  5.3477e-09,  5.3477e-09,\n",
            "        -2.6739e-09, -7.5759e-09,  2.6739e-09, -4.4564e-09, -3.5651e-09,\n",
            "        -3.5651e-09, -1.7826e-09, -1.7826e-09,  6.2390e-09, -3.5651e-09,\n",
            "         3.5651e-09,  3.1195e-09,  5.3477e-09,  3.1195e-09,  0.0000e+00,\n",
            "         7.1303e-09,  5.3477e-09,  7.1303e-09,  3.5651e-09, -2.6739e-09,\n",
            "         3.5651e-09, -8.9128e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.0695e-08, -3.1195e-09, -1.7826e-09, -7.1303e-09, -7.1303e-09,\n",
            "         1.0695e-08,  1.7826e-09, -1.7826e-09,  5.3477e-09, -8.9128e-09,\n",
            "        -3.5651e-09, -7.1303e-09,  3.5651e-09])\n",
            "PASSED: All means are less than 10**-6\n"
          ]
        }
      ],
      "source": [
        "# Normalised arrays\n",
        "normalised_embeddings_byols = speaker_normalisation(embeddings_array_byols, speakers)\n",
        "normalised_embeddings_compare= speaker_normalisation(embeddings_array_compare, speakers)\n",
        "normalised_embeddings_egemaps = speaker_normalisation(embeddings_array_egemaps, speakers)\n",
        "\n",
        "\n",
        "# Verifying normalised_embeddings_arrays\n",
        "\n",
        "normalised_embeddings_arrays = {'hybrid_byols': normalised_embeddings_byols, 'compare':normalised_embeddings_compare, 'egemaps':normalised_embeddings_egemaps}\n",
        "\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of the normalised embeddings array is: {}'.format(normalised_embeddings_arrays[model].shape))\n",
        "  print('Normalised Embeddings Array:')\n",
        "  print((normalised_embeddings_arrays[model]))\n",
        "  columnwise_mean = torch.mean(normalised_embeddings_arrays[model], 0)\n",
        "  print('Columnwise_mean:')\n",
        "  print(columnwise_mean)\n",
        "  if torch.all(columnwise_mean < 10**(-6)):\n",
        "    print('PASSED: All means are less than 10**-6')\n",
        "  else:\n",
        "    print('FAILED: All means are NOT less than 10**-6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of_JsmJIX34u"
      },
      "source": [
        "### Train Test splitting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP4XPE0TS42u"
      },
      "outputs": [],
      "source": [
        "# Defining a function for splitting into train set and test set with diferent speakers in each set\n",
        "\n",
        "def split_train_test(normalised_embeddings_array, labels, speakers, test_size = 0.30):\n",
        "  '''\n",
        "  Splits into training and testing set with different speakers\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "  normalised_embeddings_array: torch tensor\n",
        "    The tensor containing normalised embeddings \n",
        "  labels: list of strings\n",
        "    The list of emotions corresponding to audio files\n",
        "  speakers: list \n",
        "    The list of speakers\n",
        "  test_size: float \n",
        "    The fraction of embeddings and labels to put in the test set\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  X_train: torch tensor\n",
        "    The normalised embeddings that will be used for training\n",
        "  X_test: torch tensor\n",
        "    The normalised embeddings that will be used for testing\n",
        "  y_train: list\n",
        "   The labels that will be used for training\n",
        "  y_test: list\n",
        "   The labels that will be used for testing\n",
        "\n",
        "  '''\n",
        "  # unique speakers in this dataset\n",
        "  all_speakers = set(speakers)\n",
        "  # unique speakers in test set\n",
        "  test_speakers = sample(all_speakers, int(test_size*len(all_speakers)))\n",
        "\n",
        "  test_speakers_indices = []\n",
        "  train_speakers_indices = []\n",
        "\n",
        "  for speaker in all_speakers:\n",
        "      if speaker in test_speakers:\n",
        "          speaker_indices = np.where(np.array(speakers)==speaker)[0]\n",
        "          test_speakers_indices.extend(speaker_indices)\n",
        "      else:\n",
        "          speaker_indices = np.where(np.array(speakers)==speaker)[0]\n",
        "          train_speakers_indices.extend(speaker_indices)\n",
        "\n",
        "  X_train = normalised_embeddings_array[train_speakers_indices]\n",
        "  X_test = normalised_embeddings_array[test_speakers_indices]\n",
        "\n",
        "  y_train = [0 for i in range(len(train_speakers_indices))]\n",
        "  y_test = [0 for i in range(len(test_speakers_indices))]\n",
        "\n",
        "  for i,index in enumerate(train_speakers_indices):\n",
        "      y_train[i] = labels[index]\n",
        "  for i,index in enumerate(test_speakers_indices):\n",
        "      y_test[i] = labels[index]\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-j5yIb13fsl"
      },
      "source": [
        "### Train Test splitting on EmoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2mDMlFD4bG",
        "outputId": "4fb4d2e1-bb91-4d16-ad9b-60e51a6bfa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of X_train is: torch.Size([368, 2048])\n",
            "X_train:\n",
            "tensor([[-1.6433,  0.4363, -0.5681,  ...,  0.1022, -0.6459,  0.0323],\n",
            "        [ 1.5184, -0.4476,  0.6933,  ...,  0.0404, -0.0185,  0.5765],\n",
            "        [-0.9973, -1.9792, -2.0397,  ..., -0.7453, -0.4625,  0.5128],\n",
            "        ...,\n",
            "        [ 0.2091,  0.2490, -2.3426,  ...,  1.7335,  1.7397, -1.2892],\n",
            "        [-0.6214,  0.0556,  0.4667,  ...,  0.0756, -1.3667,  0.3865],\n",
            "        [-0.0228,  0.4206, -1.4976,  ...,  0.7505,  0.6567,  1.0557]])\n",
            "\n",
            "The shape of X_test is: torch.Size([167, 2048])\n",
            "X_test:\n",
            "tensor([[ 0.6146,  3.7346, -0.2716,  ...,  0.4125, -1.6326,  0.9995],\n",
            "        [-0.2403, -1.2074,  0.7718,  ...,  0.7722,  1.0866, -1.1885],\n",
            "        [-0.0483,  0.6215,  0.0361,  ...,  0.0805, -0.4190,  0.2383],\n",
            "        ...,\n",
            "        [ 0.1686,  1.0368, -0.5102,  ...,  0.3500,  1.8225, -0.9022],\n",
            "        [-1.6562, -0.2163, -1.9813,  ..., -0.1907,  0.7741,  0.9494],\n",
            "        [ 0.2894,  0.1747, -0.6020,  ...,  0.2717,  1.8460, -0.1170]])\n",
            "\n",
            "The length of y_train is: 368\n",
            "y_train:\n",
            "['L', 'F', 'L', 'T', 'L', 'W', 'N', 'F', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'N', 'W', 'N', 'N', 'L', 'N', 'T', 'L', 'W', 'A', 'A', 'F', 'F', 'E', 'L', 'F', 'W', 'T', 'A', 'L', 'W', 'W', 'E', 'E', 'L', 'W', 'N', 'W', 'A', 'E', 'N', 'W', 'F', 'W', 'N', 'W', 'T', 'L', 'N', 'N', 'E', 'T', 'N', 'W', 'W', 'L', 'L', 'F', 'T', 'L', 'N', 'W', 'N', 'W', 'W', 'L', 'T', 'N', 'W', 'T', 'W', 'T', 'F', 'N', 'A', 'W', 'W', 'F', 'A', 'A', 'N', 'T', 'E', 'F', 'W', 'W', 'F', 'W', 'N', 'N', 'T', 'W', 'A', 'L', 'N', 'F', 'N', 'W', 'N', 'F', 'T', 'L', 'A', 'F', 'W', 'N', 'W', 'L', 'F', 'W', 'E', 'E', 'T', 'E', 'W', 'E', 'L', 'W', 'E', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'E', 'L', 'T', 'E', 'L', 'T', 'W', 'N', 'N', 'N', 'W', 'F', 'T', 'W', 'W', 'W', 'W', 'L', 'F', 'N', 'A', 'A', 'F', 'A', 'E', 'W', 'T', 'A', 'F', 'L', 'L', 'W', 'T', 'A', 'T', 'A', 'A', 'F', 'W', 'F', 'T', 'N', 'T', 'W', 'N', 'T', 'N', 'W', 'N', 'W', 'E', 'N', 'W', 'W', 'E', 'L', 'L', 'E', 'T', 'F', 'E', 'L', 'F', 'T', 'T', 'W', 'W', 'W', 'N', 'W', 'N', 'N', 'W', 'L', 'W', 'N', 'E', 'W', 'W', 'E', 'N', 'E', 'F', 'A', 'E', 'F', 'E', 'E', 'L', 'W', 'A', 'A', 'N', 'A', 'N', 'A', 'T', 'A', 'F', 'N', 'F', 'F', 'N', 'W', 'T', 'T', 'N', 'L', 'W', 'L', 'F', 'L', 'W', 'T', 'N', 'W', 'L', 'N', 'A', 'N', 'W', 'A', 'L', 'F', 'A', 'F', 'T', 'A', 'L', 'W', 'N', 'W', 'W', 'A', 'F', 'W', 'T', 'W', 'F', 'T', 'L', 'L', 'A', 'L', 'W', 'W', 'L', 'T', 'N', 'T', 'F', 'A', 'A', 'W', 'N', 'E', 'F', 'A', 'L', 'W', 'N', 'A', 'W', 'T', 'A', 'L', 'L', 'N', 'W', 'F', 'L', 'W', 'A', 'A', 'W', 'L', 'W', 'F', 'W', 'A', 'W', 'N', 'L', 'F', 'L', 'L', 'W', 'W', 'N', 'W', 'F', 'N', 'W', 'W', 'A', 'L', 'T', 'A', 'A', 'A', 'F', 'N', 'F', 'L', 'F', 'F', 'T', 'N', 'L', 'W', 'T', 'W', 'L', 'A', 'N', 'T', 'W', 'T', 'L', 'T', 'N', 'L', 'N', 'F', 'N', 'N', 'W', 'F', 'F', 'L', 'T', 'W', 'T', 'T', 'F', 'W', 'F']\n",
            "\n",
            "The length of y_test is: 167\n",
            "y_test:\n",
            "['A', 'W', 'W', 'E', 'A', 'W', 'W', 'W', 'W', 'T', 'N', 'L', 'W', 'L', 'A', 'T', 'L', 'W', 'A', 'F', 'T', 'A', 'L', 'E', 'N', 'L', 'W', 'N', 'F', 'W', 'W', 'W', 'N', 'T', 'A', 'E', 'E', 'W', 'W', 'W', 'L', 'T', 'L', 'W', 'E', 'A', 'W', 'W', 'T', 'F', 'F', 'T', 'E', 'L', 'A', 'T', 'E', 'L', 'W', 'L', 'F', 'F', 'N', 'E', 'F', 'T', 'W', 'L', 'E', 'A', 'W', 'F', 'N', 'L', 'T', 'L', 'E', 'L', 'W', 'N', 'L', 'N', 'A', 'A', 'W', 'F', 'T', 'W', 'F', 'T', 'W', 'L', 'F', 'A', 'L', 'F', 'T', 'E', 'F', 'L', 'E', 'W', 'E', 'L', 'N', 'A', 'A', 'T', 'T', 'E', 'T', 'L', 'T', 'A', 'T', 'E', 'L', 'F', 'W', 'N', 'F', 'A', 'L', 'W', 'A', 'L', 'L', 'W', 'N', 'N', 'L', 'F', 'F', 'W', 'E', 'A', 'E', 'L', 'A', 'F', 'N', 'W', 'W', 'F', 'N', 'W', 'L', 'N', 'W', 'F', 'N', 'L', 'F', 'L', 'W', 'F', 'A', 'E', 'W', 'E', 'F', 'W', 'W', 'E', 'E', 'N', 'N']\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of X_train is: torch.Size([337, 6373])\n",
            "X_train:\n",
            "tensor([[-0.8715, -1.2047,  1.4478,  ..., -0.5492, -0.4581, -0.9697],\n",
            "        [ 0.8333, -1.2812,  1.4323,  ...,  1.5505,  1.9655,  1.0238],\n",
            "        [-1.0532, -1.5629,  1.1496,  ..., -0.7196, -0.8851, -0.7061],\n",
            "        ...,\n",
            "        [ 0.7761,  1.3089, -0.9914,  ..., -0.3269,  1.4607,  0.6993],\n",
            "        [-0.0470, -0.6471,  1.4149,  ...,  1.0131,  0.2241, -0.2477],\n",
            "        [ 1.8502, -0.1030, -1.0089,  ...,  1.7746,  1.3153,  3.2315]])\n",
            "\n",
            "The shape of X_test is: torch.Size([198, 6373])\n",
            "X_test:\n",
            "tensor([[-0.7592, -0.5905,  0.4798,  ..., -1.7397, -1.2616, -0.9612],\n",
            "        [-0.9152, -1.0748,  1.3454,  ...,  0.0681, -1.0823, -0.8735],\n",
            "        [-0.4711, -0.5369,  1.1264,  ...,  1.2888, -0.0484, -0.0093],\n",
            "        ...,\n",
            "        [-0.1654, -0.6885, -0.9474,  ..., -0.7817,  0.1068,  0.9966],\n",
            "        [ 0.8633,  1.2265, -0.3336,  ...,  1.5094,  1.2880,  1.4901],\n",
            "        [ 0.4123,  1.4933, -0.8997,  ..., -1.1244, -0.4278, -0.3012]])\n",
            "\n",
            "The length of y_train is: 337\n",
            "y_train:\n",
            "['L', 'F', 'L', 'T', 'L', 'W', 'N', 'F', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'N', 'W', 'N', 'N', 'L', 'N', 'T', 'L', 'W', 'A', 'A', 'F', 'F', 'E', 'L', 'F', 'W', 'T', 'A', 'L', 'W', 'W', 'E', 'E', 'L', 'W', 'N', 'W', 'A', 'E', 'N', 'W', 'F', 'W', 'N', 'W', 'T', 'L', 'N', 'N', 'E', 'T', 'N', 'W', 'W', 'L', 'L', 'F', 'T', 'L', 'N', 'W', 'N', 'W', 'W', 'L', 'T', 'N', 'W', 'T', 'W', 'T', 'F', 'N', 'A', 'W', 'W', 'F', 'A', 'A', 'N', 'T', 'E', 'F', 'W', 'W', 'F', 'W', 'N', 'N', 'T', 'W', 'A', 'L', 'N', 'F', 'N', 'W', 'N', 'F', 'A', 'W', 'W', 'E', 'A', 'W', 'W', 'W', 'W', 'T', 'N', 'L', 'W', 'L', 'A', 'T', 'L', 'W', 'A', 'F', 'T', 'A', 'L', 'E', 'N', 'L', 'W', 'N', 'F', 'W', 'W', 'W', 'N', 'T', 'A', 'W', 'N', 'T', 'N', 'W', 'N', 'W', 'E', 'N', 'W', 'W', 'E', 'L', 'L', 'E', 'T', 'F', 'E', 'L', 'F', 'T', 'T', 'W', 'W', 'W', 'N', 'W', 'N', 'N', 'W', 'L', 'W', 'N', 'E', 'W', 'W', 'E', 'N', 'E', 'F', 'A', 'E', 'F', 'E', 'E', 'L', 'W', 'A', 'A', 'N', 'A', 'N', 'A', 'T', 'A', 'F', 'N', 'F', 'F', 'N', 'W', 'T', 'T', 'N', 'L', 'W', 'L', 'F', 'L', 'W', 'T', 'N', 'W', 'L', 'N', 'A', 'N', 'W', 'A', 'L', 'F', 'A', 'F', 'T', 'A', 'L', 'W', 'N', 'W', 'W', 'A', 'F', 'W', 'T', 'W', 'F', 'T', 'L', 'A', 'T', 'T', 'E', 'T', 'L', 'T', 'A', 'T', 'E', 'L', 'F', 'W', 'N', 'F', 'A', 'L', 'W', 'A', 'L', 'L', 'W', 'N', 'N', 'L', 'F', 'F', 'W', 'E', 'A', 'E', 'L', 'A', 'F', 'N', 'W', 'W', 'F', 'N', 'W', 'L', 'N', 'W', 'F', 'N', 'L', 'F', 'L', 'W', 'F', 'A', 'E', 'W', 'E', 'F', 'W', 'W', 'E', 'E', 'N', 'N', 'L', 'A', 'L', 'W', 'W', 'L', 'T', 'N', 'T', 'F', 'A', 'A', 'W', 'N', 'E', 'F', 'A', 'L', 'W', 'N', 'A', 'W', 'T', 'A', 'L', 'L', 'N', 'W', 'F', 'L', 'W', 'A', 'A', 'W', 'L', 'W', 'F', 'W']\n",
            "\n",
            "The length of y_test is: 198\n",
            "y_test:\n",
            "['T', 'L', 'A', 'F', 'W', 'N', 'W', 'L', 'F', 'W', 'E', 'E', 'T', 'E', 'W', 'E', 'L', 'W', 'E', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'E', 'L', 'T', 'E', 'L', 'T', 'W', 'N', 'N', 'N', 'W', 'F', 'T', 'W', 'W', 'W', 'W', 'L', 'F', 'N', 'A', 'A', 'F', 'A', 'E', 'W', 'T', 'A', 'F', 'L', 'L', 'W', 'T', 'A', 'T', 'A', 'A', 'F', 'W', 'F', 'T', 'N', 'T', 'E', 'E', 'W', 'W', 'W', 'L', 'T', 'L', 'W', 'E', 'A', 'W', 'W', 'T', 'F', 'F', 'T', 'E', 'L', 'A', 'T', 'E', 'L', 'W', 'L', 'F', 'F', 'N', 'E', 'F', 'T', 'W', 'L', 'E', 'A', 'W', 'F', 'N', 'L', 'T', 'L', 'E', 'L', 'W', 'N', 'L', 'N', 'A', 'A', 'W', 'F', 'T', 'W', 'F', 'T', 'W', 'L', 'F', 'A', 'L', 'F', 'T', 'E', 'F', 'L', 'E', 'W', 'E', 'L', 'N', 'A', 'A', 'W', 'N', 'L', 'F', 'L', 'L', 'W', 'W', 'N', 'W', 'F', 'N', 'W', 'W', 'A', 'L', 'T', 'A', 'A', 'A', 'F', 'N', 'F', 'L', 'F', 'F', 'T', 'N', 'L', 'W', 'T', 'W', 'L', 'A', 'N', 'T', 'W', 'T', 'L', 'T', 'N', 'L', 'N', 'F', 'N', 'N', 'W', 'F', 'F', 'L', 'T', 'W', 'T', 'T', 'F', 'W', 'F']\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of X_train is: torch.Size([372, 88])\n",
            "X_train:\n",
            "tensor([[-0.9527,  0.0308, -0.9468,  ...,  2.1341,  4.5762,  0.5868],\n",
            "        [ 1.4543, -0.3315,  1.5677,  ..., -0.6234, -0.3017, -0.5923],\n",
            "        [-1.1822, -0.5124, -1.0540,  ..., -0.0306, -0.2251,  1.8494],\n",
            "        ...,\n",
            "        [-0.3597, -0.2104, -0.3040,  ..., -0.7621, -0.6915, -0.7924],\n",
            "        [ 0.6874,  0.0295,  0.7660,  ..., -0.1506, -0.3389, -0.9341],\n",
            "        [-0.1915,  1.6517, -0.3309,  ...,  0.6587, -0.1683,  1.4103]])\n",
            "\n",
            "The shape of X_test is: torch.Size([163, 88])\n",
            "X_test:\n",
            "tensor([[-1.0602,  2.2180, -1.1086,  ...,  2.2235,  3.6607, -0.9170],\n",
            "        [-0.5905,  0.9669, -1.0944,  ..., -0.9135, -0.4717,  0.4132],\n",
            "        [ 1.0911, -0.6578,  1.1773,  ..., -0.6022, -0.0768, -1.6917],\n",
            "        ...,\n",
            "        [ 0.7732, -0.8608,  0.8319,  ..., -1.3019, -1.0809,  1.7890],\n",
            "        [-0.1343,  0.5710, -0.2055,  ...,  1.4601,  1.4283,  0.4288],\n",
            "        [ 1.4514, -0.3461,  1.4473,  ..., -1.3954, -0.9849, -1.0952]])\n",
            "\n",
            "The length of y_train is: 372\n",
            "y_train:\n",
            "['L', 'F', 'L', 'T', 'L', 'W', 'N', 'F', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'N', 'W', 'N', 'N', 'L', 'N', 'T', 'L', 'W', 'A', 'A', 'F', 'F', 'E', 'L', 'F', 'W', 'T', 'A', 'L', 'W', 'W', 'E', 'E', 'L', 'W', 'N', 'W', 'A', 'E', 'N', 'W', 'F', 'W', 'N', 'W', 'T', 'L', 'N', 'N', 'E', 'T', 'L', 'A', 'F', 'W', 'N', 'W', 'L', 'F', 'W', 'E', 'E', 'T', 'E', 'W', 'E', 'L', 'W', 'E', 'A', 'W', 'A', 'W', 'A', 'N', 'A', 'E', 'L', 'T', 'E', 'L', 'T', 'W', 'N', 'N', 'N', 'W', 'F', 'T', 'W', 'W', 'W', 'W', 'L', 'F', 'N', 'A', 'A', 'F', 'A', 'E', 'W', 'T', 'A', 'F', 'L', 'L', 'W', 'T', 'A', 'T', 'A', 'A', 'F', 'W', 'F', 'T', 'N', 'T', 'A', 'W', 'W', 'E', 'A', 'W', 'W', 'W', 'W', 'T', 'N', 'L', 'W', 'L', 'A', 'T', 'L', 'W', 'A', 'F', 'T', 'A', 'L', 'E', 'N', 'L', 'W', 'N', 'F', 'W', 'W', 'W', 'N', 'T', 'A', 'E', 'E', 'L', 'W', 'A', 'A', 'N', 'A', 'N', 'A', 'T', 'A', 'F', 'N', 'F', 'F', 'N', 'W', 'T', 'T', 'N', 'L', 'W', 'L', 'F', 'L', 'W', 'T', 'N', 'W', 'L', 'N', 'A', 'N', 'W', 'A', 'L', 'F', 'A', 'F', 'T', 'A', 'L', 'W', 'N', 'W', 'W', 'A', 'F', 'W', 'T', 'W', 'F', 'T', 'L', 'A', 'T', 'T', 'E', 'T', 'L', 'T', 'A', 'T', 'E', 'L', 'F', 'W', 'N', 'F', 'A', 'L', 'W', 'A', 'L', 'L', 'W', 'N', 'N', 'L', 'F', 'F', 'W', 'E', 'A', 'E', 'L', 'A', 'F', 'N', 'W', 'W', 'F', 'N', 'W', 'L', 'N', 'W', 'F', 'N', 'L', 'F', 'L', 'W', 'F', 'A', 'E', 'W', 'E', 'F', 'W', 'W', 'E', 'E', 'N', 'N', 'L', 'A', 'L', 'W', 'W', 'L', 'T', 'N', 'T', 'F', 'A', 'A', 'W', 'N', 'E', 'F', 'A', 'L', 'W', 'N', 'A', 'W', 'T', 'A', 'L', 'L', 'N', 'W', 'F', 'L', 'W', 'A', 'A', 'W', 'L', 'W', 'F', 'W', 'A', 'W', 'N', 'L', 'F', 'L', 'L', 'W', 'W', 'N', 'W', 'F', 'N', 'W', 'W', 'A', 'L', 'T', 'A', 'A', 'A', 'F', 'N', 'F', 'L', 'F', 'F', 'T', 'N', 'L', 'W', 'T', 'W', 'L', 'A', 'N', 'T', 'W', 'T', 'L', 'T', 'N', 'L', 'N', 'F', 'N', 'N', 'W', 'F', 'F', 'L', 'T', 'W', 'T', 'T', 'F', 'W', 'F']\n",
            "\n",
            "The length of y_test is: 163\n",
            "y_test:\n",
            "['T', 'N', 'W', 'W', 'L', 'L', 'F', 'T', 'L', 'N', 'W', 'N', 'W', 'W', 'L', 'T', 'N', 'W', 'T', 'W', 'T', 'F', 'N', 'A', 'W', 'W', 'F', 'A', 'A', 'N', 'T', 'E', 'F', 'W', 'W', 'F', 'W', 'N', 'N', 'T', 'W', 'A', 'L', 'N', 'F', 'N', 'W', 'N', 'F', 'E', 'E', 'W', 'W', 'W', 'L', 'T', 'L', 'W', 'E', 'A', 'W', 'W', 'T', 'F', 'F', 'T', 'E', 'L', 'A', 'T', 'E', 'L', 'W', 'L', 'F', 'F', 'N', 'E', 'F', 'T', 'W', 'L', 'E', 'A', 'W', 'F', 'N', 'L', 'T', 'L', 'E', 'L', 'W', 'N', 'L', 'N', 'A', 'A', 'W', 'F', 'T', 'W', 'F', 'T', 'W', 'L', 'F', 'A', 'L', 'F', 'T', 'E', 'F', 'L', 'E', 'W', 'E', 'L', 'N', 'A', 'W', 'N', 'T', 'N', 'W', 'N', 'W', 'E', 'N', 'W', 'W', 'E', 'L', 'L', 'E', 'T', 'F', 'E', 'L', 'F', 'T', 'T', 'W', 'W', 'W', 'N', 'W', 'N', 'N', 'W', 'L', 'W', 'N', 'E', 'W', 'W', 'E', 'N', 'E', 'F', 'A', 'E', 'F']\n"
          ]
        }
      ],
      "source": [
        "# Phase_3: Train Test splitting\n",
        "\n",
        "X_train_byols, X_test_byols, y_train_byols, y_test_byols = split_train_test(normalised_embeddings_byols, labels, speakers, test_size = 0.30)\n",
        "X_train_compare, X_test_compare, y_train_compare, y_test_compare = split_train_test(normalised_embeddings_compare, labels, speakers, test_size = 0.30)\n",
        "X_train_egemaps, X_test_egemaps, y_train_egemaps, y_test_egemaps = split_train_test(normalised_embeddings_egemaps, labels, speakers, test_size = 0.30)\n",
        "\n",
        "X_trains = {'hybrid_byols':X_train_byols, 'compare':X_train_compare, 'egemaps':X_train_egemaps}\n",
        "X_tests = {'hybrid_byols':X_test_byols, 'compare':X_test_compare, 'egemaps':X_test_egemaps}\n",
        "y_trains = {'hybrid_byols':y_train_byols, 'compare':y_train_compare, 'egemaps':y_train_egemaps}\n",
        "y_tests = {'hybrid_byols':y_test_byols, 'compare':y_test_compare, 'egemaps':y_test_egemaps}\n",
        "\n",
        "# Verify\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of X_train is: {}'.format(X_trains[model].shape))\n",
        "  print('X_train:')\n",
        "  print(X_trains[model])\n",
        "  print()\n",
        "  print('The shape of X_test is: {}'.format(X_tests[model].shape))\n",
        "  print('X_test:')\n",
        "  print(X_tests[model])\n",
        "  print()\n",
        "  print('The length of y_train is: {}'.format(len(y_trains[model])))\n",
        "  print('y_train:')\n",
        "  print(y_trains[model])\n",
        "  print()\n",
        "  print('The length of y_test is: {}'.format(len(y_tests[model])))\n",
        "  print('y_test:')\n",
        "  print(y_tests[model])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning functions"
      ],
      "metadata": {
        "id": "71fAXIdzcnWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLj0kbWmVZ3A"
      },
      "outputs": [],
      "source": [
        "# Defining a function for hyperparameter tuning and getting the accuracy on the test set\n",
        "\n",
        "def get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters):\n",
        "  '''\n",
        "  Splits into training and testing set with different speakers\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "  X_train: torch tensor\n",
        "    The normalised embeddings that will be used for training\n",
        "  X_test: torch tensor\n",
        "    The normalised embeddings that will be used for testing\n",
        "  y_train: list\n",
        "    The labels that will be used for training\n",
        "  y_test: list\n",
        "    The labels that will be used for testing\n",
        "  classifier: object\n",
        "    The instance of the classification model \n",
        "  parameters: dictionary\n",
        "    The dictionary of parameters for GridSearchCV \n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "    The dictionary of the best hyperparameters\n",
        "  \n",
        "  '''\n",
        "  grid = GridSearchCV(classifier, param_grid = parameters, cv=5, scoring='recall_macro')                     \n",
        "  grid.fit(X_train,y_train)\n",
        "  print('recall_macro :',grid.best_score_)\n",
        "  print('Best Parameters: {}'.format(grid.best_params_))\n",
        "  print('recall_macro on test_set: {}'.format(grid.score(X_test, y_test)))\n",
        "  predictions = grid.predict(X_test)\n",
        "  print(classification_report(y_test, predictions))\n",
        "  return grid.score(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fckGF6LiFKVI"
      },
      "source": [
        "### Hyperparameter tuning and getting recall_macro on EmoDB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {'Logistic Regression': {'hybrid_byols': 0, 'compare': 0, 'egemaps': 0},\n",
        "        'Support Vector Machine': {'hybrid_byols': 0, 'compare': 0, 'egemaps': 0},\n",
        "        'Random Forest Classification': {'hybrid_byols': 0, 'compare': 0, 'egemaps': 0}}\n",
        "        "
      ],
      "metadata": {
        "id": "nWjiiUz2O62j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSIzZ9VKWvHv",
        "outputId": "cbcbca03-ab05-4849-a5dc-9e2fb13e35d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "MODEL: hybrid_byols\n",
            "recall_macro : 0.8970017564135213\n",
            "Best Parameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "recall_macro on test_set: 0.873632866703061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.94      0.75      0.83        20\n",
            "           E       1.00      0.81      0.89        21\n",
            "           F       0.67      0.70      0.68        23\n",
            "           L       1.00      0.97      0.98        29\n",
            "           N       0.82      1.00      0.90        18\n",
            "           T       0.90      1.00      0.95        18\n",
            "           W       0.85      0.89      0.87        38\n",
            "\n",
            "    accuracy                           0.87       167\n",
            "   macro avg       0.88      0.87      0.87       167\n",
            "weighted avg       0.88      0.87      0.87       167\n",
            "\n",
            "MODEL: compare\n",
            "recall_macro : 0.8199192404234422\n",
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "recall_macro on test_set: 0.8550031328320802\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      0.72      0.84        25\n",
            "           E       0.89      0.84      0.86        19\n",
            "           F       0.82      0.60      0.69        30\n",
            "           L       0.97      0.91      0.94        32\n",
            "           N       0.79      1.00      0.88        22\n",
            "           T       0.96      0.96      0.96        28\n",
            "           W       0.74      0.95      0.83        42\n",
            "\n",
            "    accuracy                           0.86       198\n",
            "   macro avg       0.88      0.86      0.86       198\n",
            "weighted avg       0.87      0.86      0.86       198\n",
            "\n",
            "MODEL: egemaps\n",
            "recall_macro : 0.8172977675918851\n",
            "Best Parameters: {'C': 10.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "recall_macro on test_set: 0.7399444069539509\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.50      0.92      0.65        12\n",
            "           E       1.00      0.65      0.79        20\n",
            "           F       0.50      0.45      0.48        22\n",
            "           L       0.74      0.74      0.74        23\n",
            "           N       0.75      0.84      0.79        25\n",
            "           T       1.00      0.75      0.86        20\n",
            "           W       0.81      0.83      0.82        41\n",
            "\n",
            "    accuracy                           0.74       163\n",
            "   macro avg       0.76      0.74      0.73       163\n",
            "weighted avg       0.77      0.74      0.75       163\n",
            "\n",
            "\n",
            "Support Vector Machine:\n",
            "MODEL: hybrid_byols\n",
            "recall_macro : 0.8890484678719973\n",
            "Best Parameters: {'C': 10.0, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "recall_macro on test_set: 0.8788929380637285\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.89      0.80      0.84        20\n",
            "           E       0.95      0.86      0.90        21\n",
            "           F       0.70      0.70      0.70        23\n",
            "           L       1.00      0.93      0.96        29\n",
            "           N       0.82      1.00      0.90        18\n",
            "           T       0.95      1.00      0.97        18\n",
            "           W       0.85      0.87      0.86        38\n",
            "\n",
            "    accuracy                           0.87       167\n",
            "   macro avg       0.88      0.88      0.88       167\n",
            "weighted avg       0.88      0.87      0.87       167\n",
            "\n",
            "MODEL: compare\n",
            "recall_macro : 0.8116749730195109\n",
            "Best Parameters: {'C': 10.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "recall_macro on test_set: 0.8470761725742929\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      0.76      0.86        25\n",
            "           E       0.94      0.79      0.86        19\n",
            "           F       0.84      0.53      0.65        30\n",
            "           L       0.97      0.91      0.94        32\n",
            "           N       0.79      1.00      0.88        22\n",
            "           T       0.96      0.96      0.96        28\n",
            "           W       0.71      0.98      0.82        42\n",
            "\n",
            "    accuracy                           0.85       198\n",
            "   macro avg       0.89      0.85      0.85       198\n",
            "weighted avg       0.87      0.85      0.85       198\n",
            "\n",
            "MODEL: egemaps\n",
            "recall_macro : 0.8069306510482981\n",
            "Best Parameters: {'C': 0.1, 'gamma': 1e-05, 'kernel': 'linear'}\n",
            "recall_macro on test_set: 0.7516538357365504\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.45      0.83      0.59        12\n",
            "           E       0.86      0.60      0.71        20\n",
            "           F       0.67      0.55      0.60        22\n",
            "           L       0.68      0.74      0.71        23\n",
            "           N       0.84      0.84      0.84        25\n",
            "           T       1.00      0.85      0.92        20\n",
            "           W       0.83      0.85      0.84        41\n",
            "\n",
            "    accuracy                           0.76       163\n",
            "   macro avg       0.76      0.75      0.74       163\n",
            "weighted avg       0.79      0.76      0.76       163\n",
            "\n",
            "\n",
            "Random Forest Classifier:\n",
            "MODEL: hybrid_byols\n",
            "recall_macro : 0.7720234014351661\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 200}\n",
            "recall_macro on test_set: 0.757843813503202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.76      0.80      0.78        20\n",
            "           E       1.00      0.52      0.69        21\n",
            "           F       0.58      0.30      0.40        23\n",
            "           L       0.85      0.76      0.80        29\n",
            "           N       0.65      0.94      0.77        18\n",
            "           T       0.95      1.00      0.97        18\n",
            "           W       0.71      0.97      0.82        38\n",
            "\n",
            "    accuracy                           0.77       167\n",
            "   macro avg       0.79      0.76      0.75       167\n",
            "weighted avg       0.78      0.77      0.75       167\n",
            "\n",
            "MODEL: compare\n",
            "recall_macro : 0.7591645749208775\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "recall_macro on test_set: 0.7623066188197767\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.83      0.76      0.79        25\n",
            "           E       1.00      0.47      0.64        19\n",
            "           F       0.92      0.37      0.52        30\n",
            "           L       0.86      0.78      0.82        32\n",
            "           N       0.62      0.95      0.75        22\n",
            "           T       0.97      1.00      0.98        28\n",
            "           W       0.68      1.00      0.81        42\n",
            "\n",
            "    accuracy                           0.78       198\n",
            "   macro avg       0.84      0.76      0.76       198\n",
            "weighted avg       0.83      0.78      0.77       198\n",
            "\n",
            "MODEL: egemaps\n",
            "recall_macro : 0.7680116288939819\n",
            "Best Parameters: {'bootstrap': True, 'max_features': 'log2', 'n_estimators': 200}\n",
            "recall_macro on test_set: 0.7131464011421593\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.40      0.67      0.50        12\n",
            "           E       1.00      0.40      0.57        20\n",
            "           F       0.60      0.41      0.49        22\n",
            "           L       0.78      0.78      0.78        23\n",
            "           N       0.76      0.88      0.81        25\n",
            "           T       0.95      1.00      0.98        20\n",
            "           W       0.74      0.85      0.80        41\n",
            "\n",
            "    accuracy                           0.74       163\n",
            "   macro avg       0.75      0.71      0.70       163\n",
            "weighted avg       0.76      0.74      0.73       163\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "print('Logistic Regression:')\n",
        "classifier = LogisticRegression()\n",
        "parameters = {'penalty' : ['l1','l2'], 'C': np.logspace(-4,2,7), 'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  results['Logistic Regression'][model] = np.round(100*get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters),1)\n",
        "print()\n",
        "\n",
        "# Support Vector Machine\n",
        "print('Support Vector Machine:')\n",
        "classifier = SVC()\n",
        "parameters = {'C': np.logspace(-2,3,6), 'gamma': np.logspace(-5,2,8), 'kernel':['rbf','poly','sigmoid','linear']}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  results['Support Vector Machine'][model] = np.round(100*get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters),1)\n",
        "print()\n",
        "\n",
        "# Random Forest Classifier\n",
        "print('Random Forest Classifier:')\n",
        "classifier = RandomForestClassifier()\n",
        "parameters = {'n_estimators' : [50,100,200], 'max_features' : ['auto', 'log2', 'sqrt'], 'bootstrap' : [True, False]}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  results['Random Forest Classification'][model] = np.round(100*get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters),1)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "Summarising the results obtained by using Deep Learning based features (hybrid BYOL-S) and DSP based features (openSMILE comPare and openSMILE egemaps) on logistic regression, SVM, random forest classification"
      ],
      "metadata": {
        "id": "Du0u4bUWIoz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TusdAY0rphr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "54d22a27-418f-428a-c689-c3f9483e5f15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Logistic Regression  Support Vector Machine  \\\n",
              "hybrid_byols                 87.4                    87.9   \n",
              "compare                      85.5                    84.7   \n",
              "egemaps                      74.0                    75.2   \n",
              "\n",
              "              Random Forest Classification  \n",
              "hybrid_byols                          75.8  \n",
              "compare                               76.2  \n",
              "egemaps                               71.3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-402d6029-aae8-4c09-beb4-ded68bc0c517\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <th>Random Forest Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hybrid_byols</th>\n",
              "      <td>87.4</td>\n",
              "      <td>87.9</td>\n",
              "      <td>75.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compare</th>\n",
              "      <td>85.5</td>\n",
              "      <td>84.7</td>\n",
              "      <td>76.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>egemaps</th>\n",
              "      <td>74.0</td>\n",
              "      <td>75.2</td>\n",
              "      <td>71.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-402d6029-aae8-4c09-beb4-ded68bc0c517')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-402d6029-aae8-4c09-beb4-ded68bc0c517 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-402d6029-aae8-4c09-beb4-ded68bc0c517');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yJgxonzSFrS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B48qMjEB-CLk",
        "mcSr1Af5YzIH",
        "PtuDXb9-OSZU",
        "aP3vG54u9D-r",
        "Z__xn5XdPt5W",
        "91OM9gcMHWLN",
        "cS14ei0RSJb0",
        "of_JsmJIX34u",
        "4-j5yIb13fsl",
        "71fAXIdzcnWP",
        "fckGF6LiFKVI"
      ],
      "name": "speech_emotion_recognition_demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2yk+OYMjNs9/GHMU9VvkL"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}