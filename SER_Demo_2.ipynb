{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyDk4cxk1wh"
      },
      "source": [
        "# Speech Emotion Recognition Demo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48qMjEB-CLk"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hed_5ILyX0ia",
        "outputId": "85899933-f0b3-4a1d-d961-c785a1877fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-0.5.12-py3-none-any.whl (496 kB)\n",
            "\u001b[K     |████████████████████████████████| 496 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from speechbrain) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.21.6)\n",
            "Collecting torch<=1.11,>=1.7\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 9.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain) (21.3)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 28.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from speechbrain) (4.64.0)\n",
            "Collecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.11,>=1.7->speechbrain) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->speechbrain) (3.0.9)\n",
            "Collecting ruamel.yaml>=0.17.8\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->speechbrain) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (3.0.4)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 38.5 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 45.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hyperpyyaml\n",
            "  Building wheel for hyperpyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyperpyyaml: filename=HyperPyYAML-1.0.1-py3-none-any.whl size=15192 sha256=8ef0e3d29bda7d4f64d8e4540cd43bc20c8c49f73c1fe1bd4460009e8342df34\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/87/65/266d722c3932f81f16332ce842e972be8421e3a9cd3771766b\n",
            "Successfully built hyperpyyaml\n",
            "Installing collected packages: ruamel.yaml.clib, torch, ruamel.yaml, pyyaml, torchaudio, sentencepiece, hyperpyyaml, huggingface-hub, speechbrain\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.0+cu113\n",
            "    Uninstalling torchaudio-0.12.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.8.1 hyperpyyaml-1.0.1 pyyaml-6.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sentencepiece-0.1.96 speechbrain-0.5.12 torch-1.11.0 torchaudio-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.21.0\n",
            "Cloning into 'serab-byols'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 250 (delta 10), reused 16 (delta 6), pack-reused 228\u001b[K\n",
            "Receiving objects: 100% (250/250), 112.55 MiB | 31.52 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/serab-byols\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.8.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.51.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.11.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (4.64.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.9)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.0.1)\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (21.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (0.3.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (0.10.3.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->serab-byols==0.0.0) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->serab-byols==0.0.0) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa->serab-byols==0.0.0) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->serab-byols==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->serab-byols==0.0.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (2022.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->serab-byols==0.0.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa->serab-byols==0.0.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->serab-byols==0.0.0) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->serab-byols==0.0.0) (4.1.1)\n",
            "Installing collected packages: einops, serab-byols\n",
            "  Running setup.py develop for serab-byols\n",
            "Successfully installed einops-0.4.1 serab-byols-0.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tqdm==4.60.0\n",
            "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tqdm-4.60.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.4.1-py3-none-any.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 9.7 MB/s \n",
            "\u001b[?25hCollecting audinterface>=0.7.0\n",
            "  Downloading audinterface-0.9.1-py3-none-any.whl (30 kB)\n",
            "Collecting audobject>=0.6.1\n",
            "  Downloading audobject-0.7.5-py3-none-any.whl (24 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0\n",
            "  Downloading audresample-1.1.0-py3-none-any.whl (635 kB)\n",
            "\u001b[K     |████████████████████████████████| 635 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting audformat<2.0.0,>=0.12.1\n",
            "  Downloading audformat-0.14.3-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting oyaml\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Collecting audeer<2.0.0,>=1.18.0\n",
            "  Downloading audeer-1.18.0-py3-none-any.whl (20 kB)\n",
            "Collecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 61.0 MB/s \n",
            "\u001b[?25hCollecting iso3166\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (6.0)\n",
            "Collecting audiofile>=0.4.0\n",
            "  Downloading audiofile-1.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from audeer<2.0.0,>=1.18.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (4.60.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.21.6)\n",
            "Collecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from audobject>=0.6.1->opensmile) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.21)\n",
            "Building wheels for collected packages: iso-639\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=eb1be7d0e4de1d3ee34218ae3e41ed29e7ecb99690087c884600d04f1e42e4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "Successfully built iso-639\n",
            "Installing collected packages: sox, audeer, oyaml, iso3166, iso-639, audiofile, audresample, audformat, audobject, audinterface, opensmile\n",
            "Successfully installed audeer-1.18.0 audformat-0.14.3 audinterface-0.9.1 audiofile-1.1.0 audobject-0.7.5 audresample-1.1.0 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.4.1 oyaml-1.0 sox-1.4.1\n",
            "fatal: destination path 'speech_emotion_recognition' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain\n",
        "!pip install transformers\n",
        "!git clone https://github.com/GasserElbanna/serab-byols.git\n",
        "!python3 -m pip install -e ./serab-byols\n",
        "\n",
        "!pip install tqdm==4.60.0\n",
        "!pip install opensmile\n",
        "!git clone https://github.com/satvik-dixit/speech_emotion_recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTY1RZ4UwCsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef69c9b-1a24-4fc5-db04-cb1109b63a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from random import sample\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import opensmile\n",
        "import serab_byols\n",
        "from transformers import Wav2Vec2Model, HubertModel\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6Yipn9MTLg"
      },
      "source": [
        "# Phase 1: Loading and resampling audio files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a function for loading and resampling audio files"
      ],
      "metadata": {
        "id": "mcSr1Af5YzIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae5OWFUETeZC"
      },
      "outputs": [],
      "source": [
        "# Defining a function for loading and resampling audio files\n",
        "\n",
        "def load_audio_files(audio_files, resampling_frequency=16000, audio_list=None):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_files: string\n",
        "      The paths of the wav files \n",
        "  resampling_frequency: integer\n",
        "      The frequency which all audios will be resampled to\n",
        "  audio_list: list \n",
        "      The list of torch tensors of audios to which more audios need too be added, empty by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of torch tensors, one array for each audio file\n",
        "\n",
        "  '''\n",
        "  # Making audio_list\n",
        "  if audio_list is None:\n",
        "    audio_list = []\n",
        "\n",
        "  # Resampling\n",
        "  for audio in audio_files:\n",
        "    signal, fs = librosa.load(audio, sr=resampling_frequency)\n",
        "    audio_list.append(torch.from_numpy(signal))\n",
        "      \n",
        "  return audio_list\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and resampling audiofiles and collecting metadata on EmoDB dataset"
      ],
      "metadata": {
        "id": "PtuDXb9-OSZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Jncl071Q_z",
        "outputId": "fcb58ef7-027d-4ddc-f3de-7be3cc98933f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "berlin-database-of-emotional-speech-emodb.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace wav/03a01Fa.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: All\n",
            "Number of audio files: 535\n",
            "Number of speaker classes: 10\n",
            "Speaker classes: {'13', '15', '12', '08', '14', '16', '09', '10', '03', '11'}\n",
            "Number of speakers: 535\n",
            "Speakers:\n",
            "['15' '08' '13' '13' '13' '11' '13' '13' '09' '08' '16' '15' '12' '08'\n",
            " '13' '08' '13' '14' '16' '15' '03' '15' '12' '16' '12' '16' '03' '16'\n",
            " '13' '16' '11' '12' '16' '13' '03' '03' '14' '15' '16' '13' '12' '10'\n",
            " '14' '14' '09' '16' '03' '16' '03' '08' '15' '14' '16' '16' '11' '03'\n",
            " '13' '10' '11' '09' '03' '13' '10' '08' '11' '15' '03' '03' '03' '13'\n",
            " '10' '16' '13' '09' '13' '14' '10' '11' '13' '11' '13' '16' '16' '14'\n",
            " '09' '09' '09' '03' '12' '15' '08' '03' '12' '11' '12' '16' '15' '03'\n",
            " '10' '14' '13' '16' '13' '08' '12' '13' '10' '13' '08' '08' '03' '14'\n",
            " '12' '16' '15' '09' '13' '16' '13' '14' '09' '14' '10' '13' '15' '08'\n",
            " '14' '10' '14' '13' '03' '16' '09' '08' '14' '11' '13' '15' '12' '14'\n",
            " '14' '12' '14' '15' '08' '16' '15' '16' '13' '15' '16' '15' '03' '14'\n",
            " '15' '11' '14' '14' '12' '13' '14' '09' '08' '03' '14' '10' '15' '08'\n",
            " '03' '09' '08' '11' '13' '14' '15' '16' '08' '14' '13' '09' '15' '16'\n",
            " '14' '08' '10' '15' '12' '08' '13' '11' '03' '16' '10' '16' '15' '14'\n",
            " '09' '14' '10' '09' '13' '16' '14' '12' '15' '12' '13' '11' '15' '16'\n",
            " '03' '08' '15' '15' '16' '09' '11' '14' '12' '16' '16' '16' '09' '13'\n",
            " '14' '15' '11' '14' '16' '09' '08' '11' '15' '13' '12' '03' '08' '13'\n",
            " '12' '03' '15' '11' '16' '16' '09' '14' '11' '09' '09' '14' '09' '08'\n",
            " '13' '08' '08' '13' '08' '16' '08' '15' '12' '03' '10' '10' '11' '13'\n",
            " '15' '11' '16' '03' '03' '03' '14' '16' '09' '11' '12' '12' '15' '16'\n",
            " '16' '14' '08' '13' '03' '11' '15' '03' '09' '08' '10' '08' '09' '11'\n",
            " '11' '14' '11' '11' '08' '13' '09' '14' '16' '14' '03' '10' '15' '14'\n",
            " '03' '08' '15' '14' '03' '13' '15' '13' '09' '16' '14' '10' '16' '10'\n",
            " '10' '08' '13' '03' '12' '03' '08' '08' '14' '16' '14' '13' '16' '16'\n",
            " '08' '08' '16' '13' '11' '11' '09' '08' '14' '12' '15' '03' '16' '12'\n",
            " '09' '11' '03' '08' '11' '09' '11' '12' '15' '16' '08' '11' '15' '10'\n",
            " '09' '11' '09' '15' '10' '14' '10' '16' '16' '11' '08' '10' '11' '16'\n",
            " '13' '14' '16' '14' '11' '12' '16' '10' '03' '11' '10' '16' '14' '13'\n",
            " '08' '11' '11' '10' '12' '03' '13' '13' '03' '09' '15' '09' '03' '16'\n",
            " '14' '10' '13' '10' '16' '16' '10' '12' '16' '15' '11' '11' '15' '14'\n",
            " '14' '15' '16' '15' '11' '13' '10' '16' '03' '03' '03' '14' '11' '08'\n",
            " '11' '13' '11' '08' '08' '14' '10' '11' '16' '14' '13' '14' '08' '11'\n",
            " '10' '10' '12' '08' '08' '16' '14' '08' '09' '14' '03' '16' '11' '15'\n",
            " '13' '12' '14' '15' '08' '11' '13' '15' '14' '08' '15' '14' '03' '09'\n",
            " '13' '08' '15' '09' '15' '10' '14' '09' '15' '11' '15' '14' '10' '14'\n",
            " '14' '14' '14' '08' '10' '16' '12' '12' '08' '11' '08' '16' '12' '08'\n",
            " '16' '08' '08' '09' '09' '11' '10' '13' '16' '11' '14' '03' '16' '11'\n",
            " '03' '12' '13' '15' '15' '03' '13' '11' '16' '09' '15' '09' '03' '14'\n",
            " '14' '09' '13']\n",
            "Number of label classes: 7\n",
            "Label classes: {'L', 'E', 'W', 'T', 'N', 'F', 'A'}\n",
            "Number of labels: 535\n",
            "Labels:\n",
            "['F' 'L' 'L' 'N' 'A' 'L' 'W' 'N' 'W' 'A' 'F' 'N' 'N' 'W' 'L' 'A' 'F' 'A'\n",
            " 'E' 'W' 'A' 'A' 'L' 'W' 'A' 'T' 'F' 'F' 'W' 'E' 'N' 'W' 'W' 'L' 'L' 'T'\n",
            " 'N' 'T' 'E' 'T' 'A' 'W' 'N' 'L' 'N' 'W' 'W' 'W' 'W' 'L' 'L' 'A' 'A' 'L'\n",
            " 'T' 'A' 'T' 'W' 'W' 'E' 'E' 'F' 'W' 'W' 'W' 'W' 'T' 'N' 'L' 'N' 'W' 'T'\n",
            " 'E' 'L' 'T' 'T' 'W' 'W' 'F' 'L' 'A' 'T' 'F' 'A' 'T' 'E' 'N' 'L' 'E' 'W'\n",
            " 'N' 'L' 'W' 'L' 'A' 'F' 'N' 'W' 'F' 'W' 'F' 'T' 'L' 'T' 'F' 'N' 'L' 'T'\n",
            " 'T' 'N' 'F' 'T' 'L' 'E' 'A' 'T' 'W' 'E' 'F' 'W' 'E' 'L' 'T' 'L' 'E' 'L'\n",
            " 'L' 'A' 'W' 'W' 'W' 'L' 'N' 'F' 'A' 'L' 'L' 'A' 'T' 'T' 'A' 'W' 'T' 'F'\n",
            " 'T' 'W' 'T' 'F' 'L' 'W' 'F' 'T' 'F' 'F' 'E' 'N' 'T' 'W' 'T' 'A' 'A' 'N'\n",
            " 'A' 'W' 'L' 'A' 'E' 'T' 'W' 'T' 'F' 'N' 'N' 'W' 'W' 'A' 'L' 'T' 'E' 'W'\n",
            " 'W' 'F' 'E' 'F' 'W' 'L' 'F' 'W' 'N' 'A' 'W' 'N' 'L' 'L' 'A' 'A' 'N' 'W'\n",
            " 'L' 'W' 'F' 'W' 'N' 'W' 'F' 'W' 'L' 'L' 'E' 'T' 'T' 'N' 'W' 'N' 'W' 'E'\n",
            " 'A' 'F' 'A' 'N' 'E' 'E' 'L' 'W' 'E' 'A' 'T' 'L' 'F' 'E' 'T' 'E' 'T' 'E'\n",
            " 'A' 'N' 'N' 'A' 'W' 'F' 'W' 'W' 'L' 'N' 'E' 'F' 'A' 'N' 'W' 'N' 'N' 'N'\n",
            " 'E' 'N' 'A' 'F' 'W' 'W' 'F' 'L' 'W' 'F' 'W' 'A' 'N' 'W' 'W' 'T' 'L' 'N'\n",
            " 'N' 'L' 'W' 'W' 'W' 'N' 'W' 'L' 'N' 'L' 'E' 'E' 'F' 'A' 'W' 'A' 'F' 'N'\n",
            " 'W' 'T' 'T' 'T' 'A' 'N' 'N' 'W' 'L' 'W' 'W' 'W' 'W' 'W' 'A' 'F' 'T' 'N'\n",
            " 'W' 'T' 'A' 'W' 'N' 'W' 'W' 'L' 'W' 'F' 'W' 'L' 'W' 'W' 'N' 'A' 'L' 'F'\n",
            " 'A' 'W' 'W' 'N' 'L' 'L' 'W' 'F' 'F' 'T' 'E' 'F' 'L' 'L' 'F' 'W' 'F' 'F'\n",
            " 'W' 'F' 'A' 'L' 'L' 'F' 'L' 'W' 'T' 'L' 'T' 'W' 'T' 'L' 'W' 'T' 'W' 'W'\n",
            " 'A' 'W' 'A' 'F' 'L' 'A' 'W' 'N' 'N' 'N' 'A' 'L' 'N' 'A' 'W' 'L' 'N' 'A'\n",
            " 'E' 'W' 'W' 'A' 'A' 'N' 'A' 'F' 'N' 'T' 'L' 'W' 'N' 'W' 'L' 'F' 'W' 'A'\n",
            " 'W' 'F' 'F' 'E' 'W' 'F' 'F' 'N' 'W' 'W' 'T' 'W' 'L' 'T' 'A' 'T' 'N' 'W'\n",
            " 'L' 'N' 'F' 'F' 'W' 'A' 'W' 'L' 'L' 'L' 'N' 'A' 'E' 'A' 'W' 'W' 'A' 'T'\n",
            " 'T' 'F' 'L' 'N' 'E' 'L' 'T' 'L' 'L' 'F' 'T' 'E' 'W' 'L' 'T' 'W' 'W' 'A'\n",
            " 'A' 'F' 'N' 'L' 'W' 'F' 'E' 'L' 'N' 'E' 'F' 'N' 'N' 'N' 'E' 'A' 'N' 'A'\n",
            " 'E' 'A' 'F' 'F' 'N' 'N' 'T' 'W' 'W' 'W' 'F' 'N' 'N' 'F' 'E' 'F' 'L' 'A'\n",
            " 'E' 'E' 'A' 'E' 'W' 'A' 'T' 'N' 'N' 'T' 'L' 'T' 'W' 'A' 'W' 'T' 'N' 'W'\n",
            " 'E' 'A' 'N' 'E' 'F' 'W' 'L' 'W' 'L' 'W' 'F' 'N' 'W' 'T' 'N' 'E' 'N' 'L'\n",
            " 'N' 'N' 'E' 'F' 'L' 'W' 'L' 'W' 'T' 'F' 'A' 'F' 'F']\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! wget http://emodb.bilderbar.info/download/download.zip\n",
        "! unzip -q download.zip\n",
        "\n",
        "# Resample dataset\n",
        "audio_files = glob('/content/wav/*.wav')\n",
        "audio_list = load_audio_files(audio_files, resampling_frequency=16000)\n",
        "\n",
        "# Extracting metadata (speakers and labels)\n",
        "labels = np.array(list(map(lambda x: os.path.basename(x).split('.')[0][-2], audio_files)))\n",
        "speakers = np.array(list(map(lambda x: os.path.basename(x)[:2], audio_files)))\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Speakers:')\n",
        "print(speakers)\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))\n",
        "print('Labels:')\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kPY4usMGbA"
      },
      "source": [
        "# Phase 2: Embedding Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3vG54u9D-r"
      },
      "source": [
        "## Audio embeddings extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCNer33gwaMl"
      },
      "outputs": [],
      "source": [
        "# Defining a function for generating audio embedding extraction models\n",
        "\n",
        "def audio_embeddings_model(model_name):\n",
        "  '''\n",
        "  Generates model for embedding extraction \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  mode_name: string\n",
        "      The model to used, could be 'wav2vec', 'hubert', 'hybrid_byols', 'compare' or 'egemaps'\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  model: object\n",
        "      The embedding extraction model\n",
        "  '''\n",
        "  if model_name=='wav2vec2':\n",
        "    model_hub = 'facebook/wav2vec2-large-960h-lv60-self'\n",
        "    model = Wav2Vec2Model.from_pretrained(model_hub)\n",
        "  elif model_name=='hubert':\n",
        "    model_hub = 'facebook/hubert-xlarge-ll60k'\n",
        "    model = HubertModel.from_pretrained(model_hub)\n",
        "  elif model_name=='hybrid_byols':\n",
        "    model_name = 'cvt'\n",
        "    checkpoint_path = \"serab-byols/checkpoints/cvt_s1-d1-e64_s2-d1-e256_s3-d1-e512_BYOLAs64x96-osandbyolaloss6373-e100-bs256-lr0003-rs42.pth\"\n",
        "    model = serab_byols.load_model(checkpoint_path, model_name)\n",
        "  elif model_name=='compare':\n",
        "    model = opensmile.Smile(\n",
        "        feature_set=opensmile.FeatureSet.ComParE_2016,\n",
        "        feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    )\n",
        "  elif model_name=='egemaps':\n",
        "    model = opensmile.Smile(\n",
        "        feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "        feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    )\n",
        "  return model\n",
        "\n",
        "\n",
        "# Defining a function for embedding exctraction from the audio list\n",
        "\n",
        "def audio_embeddings(audio_list, model_name, model, sampling_rate=16000):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of arrays, one array for each audio file\n",
        "  model_name: string\n",
        "      The model to used, could be 'wav2vec', 'hubert', 'hybrid_byols', 'compare' or 'egemaps'\n",
        "  model: object\n",
        "      The embedding extraction model generated by audio_embeddings_model function\n",
        "  n_feats: int\n",
        "      The number of features of each audio file, 6373 for 'compare' and 88 for 'egemaps'\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  if model_name=='hybrid_byols':\n",
        "    embeddings_array = serab_byols.get_scene_embeddings(audio_list, model)\n",
        "  else:\n",
        "    embeddings_list = []\n",
        "    for i in tqdm(range(len(audio_list))):\n",
        "      if model_name=='wav2vec2' or model_name=='hubert':\n",
        "        embeddings = model(audio_list[i].reshape(1,-1)).last_hidden_state.mean(1)\n",
        "        embeddings_list.append(embeddings[0])\n",
        "      elif model_name=='compare' or model_name=='egemaps':\n",
        "        embeddings = model.process_signal(audio_list[i], sampling_rate)\n",
        "        embeddings_list.append(torch.tensor(embeddings.values[0], dtype=torch.float32))\n",
        "    embeddings_array = torch.stack(embeddings_list)\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio embeddings extraction on EmoDB"
      ],
      "metadata": {
        "id": "Z__xn5XdPt5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase_2\n",
        "\n",
        "# Wav2vec\n",
        "model = audio_embeddings_model(model_name='wav2vec')\n",
        "embeddings_array_wav2vec = audio_embeddings(audio_list, model_name='wav2vec', model=model)\n",
        "\n",
        "# Hubert\n",
        "model = audio_embeddings_model(model_name='hubert')\n",
        "embeddings_array_hubert = audio_embeddings(audio_list, model_name='hubert', model=model)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Verify Phase_2\n",
        "models = ['wav2vec2', 'hubert']\n",
        "embeddings_arrays = {'wav2vec2': embeddings_array_wav2vec, 'hubert': embeddings_array_hubert}\n",
        "\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of the embeddings array is {}'.format(embeddings_arrays[model].shape))\n",
        "  print('The embeddings array is: ')\n",
        "  print((embeddings_arrays[model]))\n"
      ],
      "metadata": {
        "id": "YoSMmJtTsqmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvmZ0RwTDXCY",
        "outputId": "5b08a58c-98d3-422f-a0ff-8db6f12b49ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings...: 100%|██████████| 535/535 [00:22<00:00, 23.56it/s]\n",
            "100%|██████████| 535/535 [00:52<00:00, 10.15it/s]\n",
            "100%|██████████| 535/535 [00:57<00:00,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: wav2vec2\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 1024])\n",
            "The embeddings array is: \n",
            "tensor([[-0.0854, -0.1182,  0.0090,  ...,  0.1121, -0.0221,  0.2053],\n",
            "        [-0.0649, -0.1166, -0.0010,  ...,  0.1056, -0.0498,  0.2029],\n",
            "        [-0.0660, -0.1250,  0.0148,  ...,  0.1073, -0.0408,  0.2018],\n",
            "        ...,\n",
            "        [-0.0661, -0.1327,  0.0590,  ...,  0.1059, -0.0196,  0.1992],\n",
            "        [-0.0289, -0.1255,  0.0099,  ...,  0.1206, -0.0237,  0.2044],\n",
            "        [-0.0488, -0.1268,  0.0219,  ...,  0.1213, -0.0271,  0.2042]])\n",
            "\n",
            "\n",
            "MODEL: hubert\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 1280])\n",
            "The embeddings array is: \n",
            "tensor([[-0.1409,  0.0715, -0.0276,  ...,  0.0794, -0.0020, -0.0408],\n",
            "        [-0.1844, -0.0501,  0.0135,  ...,  0.0842,  0.0074, -0.0492],\n",
            "        [-0.0795,  0.0288,  0.0077,  ..., -0.0329, -0.0064,  0.0352],\n",
            "        ...,\n",
            "        [-0.1168,  0.0273,  0.0328,  ...,  0.1451, -0.0298, -0.0803],\n",
            "        [-0.1545,  0.0423,  0.0530,  ..., -0.0081,  0.0088,  0.0590],\n",
            "        [-0.1284,  0.0540,  0.0945,  ..., -0.0527, -0.0118,  0.0311]])\n",
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 2048])\n",
            "The embeddings array is: \n",
            "tensor([[ 5.7497,  4.4203,  2.2153,  ...,  6.2141,  0.5686,  3.8616],\n",
            "        [ 7.2127,  5.1693,  1.4178,  ...,  5.1131, -0.7438,  3.6326],\n",
            "        [ 4.8872,  5.8356,  1.4132,  ...,  4.2586,  0.9261,  3.7216],\n",
            "        ...,\n",
            "        [ 5.0561,  3.9716,  1.1771,  ...,  3.7682, -1.1466,  3.3680],\n",
            "        [ 4.8218,  3.8905,  1.5297,  ...,  5.7751, -0.8419,  3.0622],\n",
            "        [ 6.2711,  5.5427,  2.0362,  ...,  5.3720,  0.2666,  3.5467]])\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 6373])\n",
            "The embeddings array is: \n",
            "tensor([[3.0849e+00, 7.1512e-01, 2.9070e-02,  ..., 1.0598e+02, 1.2440e+02,\n",
            "         6.8718e+01],\n",
            "        [2.6341e+00, 7.6923e-02, 5.9172e-03,  ..., 6.6394e+01, 1.2033e+02,\n",
            "         6.8614e+01],\n",
            "        [2.3814e+00, 3.2599e-01, 4.4053e-03,  ..., 5.7051e+01, 9.5732e+01,\n",
            "         5.7653e+01],\n",
            "        ...,\n",
            "        [2.8802e+00, 5.8566e-01, 1.1952e-02,  ..., 5.5807e+01, 1.7636e+02,\n",
            "         7.0096e+01],\n",
            "        [3.7686e+00, 2.2727e-01, 2.8409e-02,  ..., 8.0770e+01, 1.6817e+02,\n",
            "         8.3845e+01],\n",
            "        [2.8666e+00, 6.3205e-01, 5.4599e-01,  ..., 8.3399e+01, 1.4755e+02,\n",
            "         7.1222e+01]])\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of the embeddings array is torch.Size([535, 88])\n",
            "The embeddings array is: \n",
            "tensor([[ 3.3635e+01,  2.3025e-01,  2.5775e+01,  ...,  5.7500e-02,\n",
            "          3.2692e-02, -1.8386e+01],\n",
            "        [ 3.1965e+01,  1.3345e-01,  2.9264e+01,  ...,  1.1250e-01,\n",
            "          5.8041e-02, -1.9749e+01],\n",
            "        [ 3.2437e+01,  5.4852e-02,  3.0390e+01,  ...,  6.6000e-02,\n",
            "          2.2450e-02, -1.4390e+01],\n",
            "        ...,\n",
            "        [ 3.7927e+01,  8.4884e-02,  3.6168e+01,  ...,  9.3636e-02,\n",
            "          5.2271e-02, -1.6925e+01],\n",
            "        [ 4.4526e+01,  7.6023e-02,  4.1384e+01,  ...,  6.6667e-02,\n",
            "          3.2998e-02, -1.9150e+01],\n",
            "        [ 3.7171e+01,  1.4792e-01,  3.2615e+01,  ...,  1.1500e-01,\n",
            "          9.4789e-02, -1.8577e+01]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# # Phase_2\n",
        "\n",
        "# # Hybrid BYOLS\n",
        "# model = audio_embeddings_model(model_name='hybrid_byols')\n",
        "# embeddings_array_byols = audio_embeddings(audio_list, model_name='hybrid_byols', model=model)\n",
        "\n",
        "# # EmoDB compare\n",
        "# model = audio_embeddings_model(model_name='compare')\n",
        "# embeddings_array_compare = audio_embeddings(audio_list, model_name='compare', model=model)\n",
        "\n",
        "# # EmoDB egemaps\n",
        "# model = audio_embeddings_model(model_name='egemaps')\n",
        "# embeddings_array_egemaps = audio_embeddings(audio_list, model_name='egemaps', model=model)\n",
        "\n",
        "# # ---------------------------------------------------------------------------------------------------\n",
        "\n",
        "# # Verify Phase_2\n",
        "# models = ['hybrid_byols', 'compare', 'egemaps']\n",
        "# embeddings_arrays = {'hybrid_byols': embeddings_array_byols, 'compare':embeddings_array_compare, 'egemaps':embeddings_array_egemaps}\n",
        "\n",
        "# for model in models:\n",
        "#   print()\n",
        "#   print()\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   print()\n",
        "#   print('The shape of the embeddings array is {}'.format(embeddings_arrays[model].shape))\n",
        "#   print('The embeddings array is: ')\n",
        "#   print((embeddings_arrays[model]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIMNm01HGyZ"
      },
      "source": [
        "# Phase 3: Downstream Task - Speech Emotion Recognotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91OM9gcMHWLN"
      },
      "source": [
        "## Speaker normalisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlFL7hMDwiqk"
      },
      "outputs": [],
      "source": [
        "# Defining a function for speaker normalisation using standard scaler\n",
        "\n",
        "def speaker_normalisation(embeddings_array, speakers):\n",
        "  '''\n",
        "  Normalises embeddings_array for each speaker\n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array of embeddings, one row for each audio file\n",
        "  speakers: list \n",
        "      The list of speakers\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg normalised embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  speaker_ids = set(speakers)\n",
        "  for speaker_id in speaker_ids:\n",
        "    speaker_embeddings_indices = np.where(np.array(speakers)==speaker_id)[0]\n",
        "    speaker_embeddings = embeddings_array[speaker_embeddings_indices,:]\n",
        "    scaler = StandardScaler()\n",
        "    normalised_speaker_embeddings = scaler.fit_transform(speaker_embeddings)\n",
        "    embeddings_array[speaker_embeddings_indices] = torch.tensor(normalised_speaker_embeddings).float()\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker normalisation on EmoDB"
      ],
      "metadata": {
        "id": "cS14ei0RSJb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalised arrays\n",
        "normalised_embeddings_wav2vec = speaker_normalisation(embeddings_array_wav2vec, speakers)\n",
        "normalised_embeddings_hubert = speaker_normalisation(embeddings_array_hubert, speakers)\n",
        "\n",
        "# Verifying normalised_embeddings_arrays\n",
        "\n",
        "models = ['wav2vec2', 'hubert']\n",
        "normalised_embeddings_arrays = {'wav2vec2': normalised_embeddings_wav2vec, 'hubert': normalised_embeddings_hubert}\n",
        "\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of the normalised embeddings array is: {}'.format(normalised_embeddings_arrays[model].shape))\n",
        "  print('Normalised Embeddings Array:')\n",
        "  print((normalised_embeddings_arrays[model]))\n",
        "  print()\n",
        "  columnwise_mean = torch.mean(normalised_embeddings_arrays[model], 0)\n",
        "  print('Columnwise_mean:')\n",
        "  print(columnwise_mean)\n",
        "  if torch.all(columnwise_mean < 10**(-6)):\n",
        "    print('All means are less than 10**-6')\n",
        "  else:\n",
        "    print('All means are NOT less than 10**-6')"
      ],
      "metadata": {
        "id": "Fh0KWGnV3Xmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uibu-ZnCDg_D",
        "outputId": "aeddb4ac-9673-437b-e5c1-7f9c041c9a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: wav2vec2\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 1024])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[-1.0630,  0.8774, -0.3200,  ..., -0.6530,  0.6994,  0.7176],\n",
            "        [-0.3161,  1.1851, -0.2382,  ..., -0.9840, -1.8121, -0.5755],\n",
            "        [-0.3934,  0.2471,  0.2027,  ..., -0.9753, -1.1490, -1.0656],\n",
            "        ...,\n",
            "        [-0.2436, -0.8298,  2.0756,  ..., -1.1859,  0.9948, -2.2049],\n",
            "        [ 1.0960,  0.1356,  0.0916,  ...,  0.1838,  0.6432,  0.3809],\n",
            "        [ 0.4585, -0.0177,  0.4946,  ...,  0.5492,  0.3978,  0.2731]])\n",
            "\n",
            "Columnwise_mean:\n",
            "tensor([ 2.2282e-10, -5.3477e-09,  0.0000e+00,  ...,  0.0000e+00,\n",
            "        -6.2390e-09,  0.0000e+00])\n",
            "All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: hubert\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 1280])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[ 0.1948,  1.3250, -1.5221,  ...,  0.4631, -0.3612, -0.1844],\n",
            "        [-0.5785, -1.7244, -0.6863,  ...,  0.5426,  0.0418, -0.0914],\n",
            "        [ 1.5018,  0.5686, -0.9444,  ..., -1.5130, -0.8435,  1.5040],\n",
            "        ...,\n",
            "        [ 0.7630,  0.4261, -0.3888,  ...,  1.2363, -3.1035, -0.7761],\n",
            "        [ 0.1014,  1.0543, -0.0164,  ..., -1.0770,  0.2301,  2.2073],\n",
            "        [ 0.5763,  1.1648,  0.7331,  ..., -1.8273, -1.2697,  1.4198]])\n",
            "\n",
            "Columnwise_mean:\n",
            "tensor([-5.3477e-09,  0.0000e+00, -3.5651e-09,  ...,  4.2336e-09,\n",
            "         1.6043e-08,  7.1303e-09])\n",
            "All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 2048])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[ 1.4272,  0.1554,  2.5108,  ...,  2.0119, -0.0082,  0.5099],\n",
            "        [ 1.3333, -0.1954, -0.0798,  ...,  0.1485, -0.6004, -0.3401],\n",
            "        [-0.4688,  0.5001, -0.1864,  ..., -1.2090,  1.2901,  0.0915],\n",
            "        ...,\n",
            "        [-0.1245, -0.4252, -0.3237,  ..., -1.1635, -0.6520,  0.1275],\n",
            "        [-0.1952, -0.8519,  0.2247,  ...,  1.4053, -0.7335, -0.8930],\n",
            "        [ 0.6512,  0.2961,  1.0938,  ...,  0.6801,  0.6363, -0.3435]])\n",
            "\n",
            "Columnwise_mean:\n",
            "tensor([ 2.8967e-09, -1.7826e-09,  4.4564e-09,  ...,  5.1249e-09,\n",
            "        -1.1587e-08, -8.9128e-10])\n",
            "All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 6373])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[-0.1223,  1.1162, -0.9038,  ...,  1.8530,  0.0681,  0.4053],\n",
            "        [-0.9238, -0.8732, -1.0182,  ..., -0.4035, -0.5868, -0.3429],\n",
            "        [-0.9259, -0.1495, -0.8701,  ..., -0.7136, -1.5548, -0.7343],\n",
            "        ...,\n",
            "        [-0.4154,  0.9362, -0.9283,  ..., -1.0639,  1.4623, -0.0804],\n",
            "        [ 1.0029, -0.7982, -0.9223,  ...,  0.5629,  1.0848,  0.6952],\n",
            "        [-0.0140,  0.8859,  0.4580,  ...,  0.9137,  0.4906,  0.0945]])\n",
            "\n",
            "Columnwise_mean:\n",
            "tensor([ 8.0216e-09,  0.0000e+00, -4.4564e-10,  ..., -3.5651e-09,\n",
            "         4.2336e-09, -3.5651e-09])\n",
            "All means are less than 10**-6\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of the normalised embeddings array is: torch.Size([535, 88])\n",
            "Normalised Embeddings Array:\n",
            "tensor([[ 0.8492,  1.6359,  0.1064,  ..., -1.1149, -0.5976, -0.1139],\n",
            "        [-0.8912,  0.5179, -0.8494,  ..., -0.2025, -0.3749, -1.0307],\n",
            "        [-0.8863, -0.9552, -0.7365,  ..., -0.8289, -0.7315,  1.1737],\n",
            "        ...,\n",
            "        [ 0.4652, -0.6618,  0.6195,  ..., -0.3179, -0.4723,  0.8702],\n",
            "        [ 1.7489, -0.5175,  1.6248,  ..., -0.7723, -0.6185, -0.3803],\n",
            "        [ 0.1585,  0.7351, -0.2658,  ...,  0.2108,  0.3438, -0.9281]])\n",
            "\n",
            "Columnwise_mean:\n",
            "tensor([ 0.0000e+00,  8.9128e-09, -3.1195e-09, -7.3531e-09, -4.4564e-09,\n",
            "        -5.3477e-09,  8.0216e-09,  7.1303e-09, -8.9128e-10,  1.7826e-09,\n",
            "         1.7826e-09,  8.9128e-09,  3.5651e-09,  1.7826e-09, -8.9128e-10,\n",
            "         1.7826e-09, -6.9075e-09,  7.5759e-09, -4.4564e-10,  0.0000e+00,\n",
            "         0.0000e+00,  1.0250e-08, -1.3369e-09,  1.3369e-09, -4.0108e-09,\n",
            "         0.0000e+00, -1.7826e-09, -8.0216e-09, -1.2032e-08, -5.3477e-09,\n",
            "         6.2390e-09,  1.1141e-10,  4.7907e-09,  1.1141e-09,  1.7826e-09,\n",
            "        -2.6739e-09,  8.9128e-10,  1.1141e-08, -3.5651e-09, -4.9021e-09,\n",
            "        -5.3477e-09, -2.6739e-09, -1.7826e-09,  1.8383e-09, -5.5705e-09,\n",
            "        -8.9128e-10,  7.1303e-09, -8.0216e-09, -2.4510e-09,  5.3477e-09,\n",
            "         2.6739e-09,  2.6739e-09,  3.5651e-09, -3.5651e-09,  3.5651e-09,\n",
            "        -1.5152e-08,  7.3531e-09, -2.8967e-09,  2.2282e-09,  1.7826e-09,\n",
            "        -4.2336e-09,  4.4564e-09, -2.6739e-09,  1.7826e-09, -8.9128e-10,\n",
            "         8.9128e-10,  0.0000e+00,  3.5651e-09,  1.7826e-09, -2.2282e-09,\n",
            "        -5.3477e-09,  1.7826e-09, -1.0695e-08,  8.9128e-10,  3.5651e-09,\n",
            "         8.9128e-09,  1.7826e-09,  0.0000e+00, -3.5651e-09,  4.4564e-09,\n",
            "         1.7826e-09,  6.6846e-09,  1.7826e-09,  0.0000e+00,  3.5651e-09,\n",
            "        -2.6739e-09,  8.9128e-10,  8.9128e-10])\n",
            "All means are less than 10**-6\n"
          ]
        }
      ],
      "source": [
        "# # Normalised arrays\n",
        "# normalised_embeddings_byols = speaker_normalisation(embeddings_array_byols, speakers)\n",
        "# normalised_embeddings_compare= speaker_normalisation(embeddings_array_compare, speakers)\n",
        "# normalised_embeddings_egemaps = speaker_normalisation(embeddings_array_egemaps, speakers)\n",
        "\n",
        "# # Verifying normalised_embeddings_arrays\n",
        "\n",
        "# models = ['hybrid_byols', 'compare', 'egemaps']\n",
        "# normalised_embeddings_arrays = {'hybrid_byols': normalised_embeddings_byols, 'compare':normalised_embeddings_compare, 'egemaps':normalised_embeddings_egemaps}\n",
        "\n",
        "# for model in models:\n",
        "#   print()\n",
        "#   print()\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   print()\n",
        "#   print('The shape of the normalised embeddings array is: {}'.format(normalised_embeddings_arrays[model].shape))\n",
        "#   print('Normalised Embeddings Array:')\n",
        "#   print((normalised_embeddings_arrays[model]))\n",
        "#   print()\n",
        "#   columnwise_mean = torch.mean(normalised_embeddings_arrays[model], 0)\n",
        "#   print('Columnwise_mean:')\n",
        "#   print(columnwise_mean)\n",
        "#   if torch.all(columnwise_mean < 10**(-6)):\n",
        "#     print('All means are less than 10**-6')\n",
        "#   else:\n",
        "#     print('All means are NOT less than 10**-6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of_JsmJIX34u"
      },
      "source": [
        "## Train Test splitting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP4XPE0TS42u"
      },
      "outputs": [],
      "source": [
        "# Defining a function for splitting into train set and test set with diferent speakers in each set\n",
        "\n",
        "def split_train_test(normalised_embeddings_array, labels, speakers, test_size = 0.30):\n",
        "  '''\n",
        "  Splits into training and testing set with different speakers\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "  normalised_embeddings_array: torch tensor\n",
        "    The tensor containing normalised embeddings \n",
        "  labels: list of strings\n",
        "    The list of emotions corresponding to audio files\n",
        "  speakers: list \n",
        "    The list of speakers\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  X_train: torch tensor\n",
        "    The normalised embeddings that will be used for training\n",
        "  X_test: torch tensor\n",
        "    The normalised embeddings that will be used for testing\n",
        "  y_train: list\n",
        "   The labels that will be used for training\n",
        "  y_test: list\n",
        "   The labels that will be used for testing\n",
        "\n",
        "  '''\n",
        "  # unique speakers in this dataset\n",
        "  all_speakers = set(speakers)\n",
        "  # unique speakers in test set\n",
        "  test_speakers = sample(all_speakers, int(test_size*len(all_speakers)))\n",
        "\n",
        "  test_speakers_indices = []\n",
        "  train_speakers_indices = []\n",
        "\n",
        "  for speaker in all_speakers:\n",
        "      if speaker in test_speakers:\n",
        "          speaker_indices = np.where(np.array(speakers)==speaker)[0]\n",
        "          test_speakers_indices.extend(speaker_indices)\n",
        "      else:\n",
        "          speaker_indices = np.where(np.array(speakers)==speaker)[0]\n",
        "          train_speakers_indices.extend(speaker_indices)\n",
        "\n",
        "  X_train = normalised_embeddings_array[train_speakers_indices]\n",
        "  X_test = normalised_embeddings_array[test_speakers_indices]\n",
        "\n",
        "  y_train = [0 for i in range(len(train_speakers_indices))]\n",
        "  y_test = [0 for i in range(len(test_speakers_indices))]\n",
        "\n",
        "  for i,index in enumerate(train_speakers_indices):\n",
        "      y_train[i] = labels[index]\n",
        "  for i,index in enumerate(test_speakers_indices):\n",
        "      y_test[i] = labels[index]\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-j5yIb13fsl"
      },
      "source": [
        "## Train Test splitting on EmoDB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase_3: Train Test splitting\n",
        "\n",
        "X_train_wav2vec, X_test_wav2vec, y_train_wav2vec, y_test_wav2vec = split_train_test(normalised_embeddings_wav2vec, labels, speakers, test_size = 0.30)\n",
        "X_train_hubert, X_test_hubert, y_train_hubert, y_test_hubert = split_train_test(normalised_embeddings_hubert, labels, speakers, test_size = 0.30)\n",
        "\n",
        "X_trains = {'wav2vec2': X_train_wav2vec, 'hubert': X_train_hubert}\n",
        "X_tests = {'wav2vec2': X_test_wav2vec, 'hubert': X_test_hubert}\n",
        "y_trains = {'wav2vec2': y_train_wav2vec, 'hubert': y_train_hubert}\n",
        "y_tests = {'wav2vec2': y_test_wav2vec, 'hubert': y_test_hubert}\n",
        "\n",
        "# Verify\n",
        "models = ['wav2vec2', 'hubert']\n",
        "for model in models:\n",
        "  print()\n",
        "  print()\n",
        "  print('MODEL: {}'.format(model))\n",
        "  print()\n",
        "  print('The shape of X_train is: {}'.format(X_trains[model].shape))\n",
        "  print('X_train:')\n",
        "  print(X_trains[model])\n",
        "  print()\n",
        "  print('The shape of X_test is: {}'.format(X_tests[model].shape))\n",
        "  print('X_test:')\n",
        "  print(X_tests[model])\n",
        "  print()\n",
        "  print('The length of y_train is: {}'.format(len(y_trains[model])))\n",
        "  print('y_train:')\n",
        "  print(y_trains[model])\n",
        "  print()\n",
        "  print('The length of y_test is: {}'.format(len(y_tests[model])))\n",
        "  print('y_test:')\n",
        "  print(y_tests[model])\n"
      ],
      "metadata": {
        "id": "DH6UsIJa4HDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2mDMlFD4bG",
        "outputId": "daea4479-7417-4e9a-a671-d49082b63551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MODEL: wav2vec2\n",
            "\n",
            "The shape of X_train is: torch.Size([366, 1024])\n",
            "X_train:\n",
            "tensor([[-0.3934,  0.2471,  0.2027,  ..., -0.9753, -1.1490, -1.0656],\n",
            "        [-1.2911, -1.5483, -0.1232,  ...,  0.4996,  0.8120,  0.7581],\n",
            "        [ 1.1216,  0.4389, -1.3890,  ...,  0.5894, -0.2409,  0.1047],\n",
            "        ...,\n",
            "        [ 0.8160,  1.1471, -0.7936,  ...,  0.0416, -1.0520,  0.0961],\n",
            "        [-1.1908, -1.9220,  1.3309,  ...,  0.3829, -0.1445, -0.1897],\n",
            "        [-0.4999,  0.2885,  0.4264,  ..., -0.2561,  2.3868,  0.1330]])\n",
            "\n",
            "The shape of X_test is: torch.Size([169, 1024])\n",
            "X_test:\n",
            "tensor([[-2.1918,  0.3515,  1.6224,  ...,  0.1890,  2.4706,  0.3731],\n",
            "        [ 0.1784,  0.9274, -0.9344,  ...,  0.0177, -0.9814,  1.2774],\n",
            "        [-1.0864,  1.0387,  0.5590,  ...,  0.3506,  0.0160, -1.2822],\n",
            "        ...,\n",
            "        [ 1.4930,  1.7136,  0.2420,  ...,  0.5099, -1.0656,  0.5255],\n",
            "        [ 1.1636,  0.5158, -0.3835,  ...,  0.7265, -0.0204,  0.4544],\n",
            "        [-1.3669, -0.8499,  0.2795,  ...,  0.1117, -0.4838,  0.9563]])\n",
            "\n",
            "The length of y_train is: 366\n",
            "y_train:\n",
            "['L', 'N', 'A', 'W', 'N', 'L', 'F', 'W', 'L', 'T', 'T', 'F', 'N', 'E', 'T', 'F', 'A', 'F', 'L', 'N', 'T', 'W', 'F', 'L', 'W', 'L', 'L', 'A', 'N', 'E', 'N', 'F', 'L', 'W', 'E', 'A', 'E', 'F', 'W', 'A', 'W', 'L', 'F', 'A', 'T', 'W', 'E', 'W', 'F', 'E', 'L', 'A', 'N', 'W', 'N', 'E', 'W', 'W', 'N', 'E', 'F', 'F', 'N', 'W', 'A', 'T', 'L', 'W', 'W', 'N', 'A', 'E', 'A', 'F', 'T', 'W', 'T', 'E', 'E', 'W', 'W', 'L', 'A', 'F', 'E', 'W', 'N', 'A', 'T', 'W', 'L', 'W', 'N', 'F', 'W', 'N', 'W', 'L', 'W', 'A', 'N', 'F', 'N', 'W', 'L', 'L', 'N', 'A', 'A', 'N', 'F', 'N', 'L', 'E', 'L', 'N', 'L', 'N', 'L', 'A', 'W', 'A', 'E', 'W', 'A', 'F', 'L', 'T', 'W', 'T', 'F', 'W', 'W', 'A', 'A', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'T', 'N', 'W', 'W', 'A', 'N', 'L', 'T', 'N', 'E', 'L', 'A', 'W', 'A', 'L', 'W', 'N', 'T', 'T', 'N', 'L', 'F', 'T', 'A', 'T', 'F', 'L', 'F', 'W', 'N', 'T', 'N', 'N', 'N', 'A', 'W', 'F', 'F', 'T', 'T', 'W', 'W', 'F', 'L', 'L', 'L', 'L', 'F', 'W', 'A', 'W', 'L', 'F', 'L', 'T', 'T', 'F', 'N', 'F', 'N', 'F', 'W', 'N', 'W', 'W', 'W', 'A', 'N', 'A', 'N', 'N', 'L', 'A', 'T', 'A', 'W', 'T', 'W', 'L', 'L', 'W', 'A', 'T', 'A', 'T', 'F', 'T', 'W', 'A', 'L', 'W', 'T', 'E', 'A', 'W', 'N', 'F', 'E', 'L', 'F', 'N', 'W', 'E', 'W', 'W', 'F', 'T', 'W', 'W', 'W', 'F', 'A', 'N', 'W', 'A', 'N', 'T', 'A', 'W', 'T', 'L', 'E', 'L', 'W', 'L', 'E', 'F', 'N', 'E', 'E', 'E', 'W', 'A', 'T', 'F', 'F', 'A', 'W', 'W', 'W', 'W', 'W', 'F', 'L', 'T', 'A', 'A', 'W', 'L', 'L', 'W', 'A', 'T', 'N', 'W', 'A', 'L', 'F', 'N', 'A', 'L', 'F', 'L', 'A', 'W', 'T', 'N', 'E', 'L', 'W', 'A', 'F', 'A', 'N', 'L', 'A', 'F', 'L', 'T', 'W', 'W', 'A', 'E', 'T', 'N', 'L', 'L', 'L', 'W', 'F', 'W', 'F', 'W', 'W', 'W', 'T', 'N', 'F', 'F', 'N', 'N', 'L', 'W', 'N', 'T', 'A', 'W', 'W', 'N', 'F', 'T', 'N', 'F', 'W', 'W', 'W', 'W', 'A', 'N', 'T', 'N', 'N', 'N', 'T']\n",
            "\n",
            "The length of y_test is: 169\n",
            "y_test:\n",
            "['F', 'E', 'W', 'T', 'F', 'E', 'W', 'E', 'W', 'W', 'A', 'L', 'T', 'T', 'F', 'F', 'T', 'E', 'E', 'L', 'W', 'F', 'F', 'A', 'F', 'N', 'L', 'W', 'T', 'W', 'N', 'E', 'E', 'F', 'L', 'N', 'W', 'L', 'W', 'L', 'E', 'A', 'L', 'N', 'F', 'E', 'F', 'F', 'L', 'W', 'L', 'N', 'A', 'W', 'A', 'W', 'W', 'A', 'T', 'L', 'L', 'A', 'T', 'L', 'E', 'T', 'T', 'E', 'L', 'W', 'L', 'W', 'N', 'E', 'L', 'T', 'E', 'N', 'T', 'E', 'N', 'N', 'T', 'W', 'N', 'W', 'E', 'L', 'E', 'E', 'N', 'W', 'N', 'W', 'W', 'A', 'W', 'W', 'W', 'T', 'L', 'L', 'W', 'F', 'N', 'E', 'W', 'N', 'F', 'E', 'F', 'W', 'W', 'F', 'L', 'N', 'T', 'W', 'W', 'W', 'L', 'L', 'L', 'N', 'N', 'A', 'L', 'A', 'T', 'E', 'W', 'A', 'N', 'T', 'N', 'A', 'N', 'N', 'L', 'W', 'F', 'F', 'L', 'T', 'W', 'W', 'A', 'A', 'N', 'A', 'T', 'F', 'W', 'F', 'F', 'N', 'T', 'L', 'E', 'F', 'W', 'F', 'A', 'A', 'A', 'W', 'W', 'T', 'F']\n",
            "\n",
            "\n",
            "MODEL: hubert\n",
            "\n",
            "The shape of X_train is: torch.Size([339, 1280])\n",
            "X_train:\n",
            "tensor([[ 1.5018,  0.5686, -0.9444,  ..., -1.5130, -0.8435,  1.5040],\n",
            "        [ 1.0544, -0.0759, -0.9925,  ..., -0.3575, -0.6375, -0.3053],\n",
            "        [-1.5258, -1.4465, -0.9143,  ...,  1.5508, -0.8296, -0.3165],\n",
            "        ...,\n",
            "        [-0.4760, -0.0854,  1.4269,  ...,  0.5601,  0.1933, -0.0569],\n",
            "        [-0.8906, -0.4719,  1.7245,  ...,  1.2104,  0.1664,  0.6715],\n",
            "        [ 0.2081,  0.6254,  0.3214,  ...,  0.1311, -0.4903,  0.4213]])\n",
            "\n",
            "The shape of X_test is: torch.Size([196, 1280])\n",
            "X_test:\n",
            "tensor([[ 0.1948,  1.3250, -1.5221,  ...,  0.4631, -0.3612, -0.1844],\n",
            "        [-0.2008, -1.0336,  0.6210,  ...,  0.7477,  1.1397, -0.4343],\n",
            "        [-0.2780, -0.7377, -0.5711,  ..., -2.3431, -0.3496, -0.4131],\n",
            "        ...,\n",
            "        [-0.5056,  1.5098,  1.8464,  ..., -1.1674, -0.1063,  2.1765],\n",
            "        [ 0.9354,  0.3107,  0.9732,  ..., -0.5809,  0.3780,  1.5227],\n",
            "        [ 1.2967,  1.1352,  0.7554,  ..., -1.3408, -0.6869,  0.8248]])\n",
            "\n",
            "The length of y_train is: 339\n",
            "y_train:\n",
            "['L', 'N', 'A', 'W', 'N', 'L', 'F', 'W', 'L', 'T', 'T', 'F', 'N', 'E', 'T', 'F', 'A', 'F', 'L', 'N', 'T', 'W', 'F', 'L', 'W', 'L', 'L', 'A', 'N', 'E', 'N', 'F', 'L', 'W', 'E', 'A', 'E', 'F', 'W', 'A', 'W', 'L', 'F', 'A', 'T', 'W', 'E', 'W', 'F', 'E', 'L', 'A', 'N', 'W', 'N', 'E', 'W', 'W', 'N', 'E', 'F', 'N', 'L', 'A', 'W', 'A', 'E', 'W', 'A', 'F', 'L', 'T', 'W', 'T', 'F', 'W', 'W', 'A', 'A', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'T', 'N', 'W', 'W', 'A', 'N', 'L', 'T', 'N', 'E', 'L', 'A', 'W', 'A', 'L', 'W', 'N', 'T', 'T', 'N', 'L', 'F', 'T', 'A', 'T', 'F', 'L', 'F', 'W', 'N', 'T', 'N', 'N', 'N', 'A', 'W', 'F', 'F', 'T', 'T', 'W', 'W', 'F', 'L', 'L', 'L', 'L', 'F', 'W', 'A', 'W', 'L', 'F', 'L', 'T', 'T', 'F', 'N', 'F', 'N', 'F', 'W', 'N', 'W', 'W', 'W', 'A', 'N', 'W', 'N', 'E', 'L', 'T', 'E', 'N', 'T', 'E', 'N', 'N', 'T', 'W', 'N', 'W', 'E', 'L', 'E', 'E', 'N', 'W', 'N', 'W', 'W', 'A', 'W', 'W', 'W', 'T', 'L', 'L', 'W', 'F', 'N', 'E', 'W', 'N', 'F', 'E', 'F', 'W', 'W', 'F', 'W', 'W', 'W', 'W', 'W', 'F', 'L', 'T', 'A', 'A', 'W', 'L', 'L', 'W', 'A', 'T', 'N', 'W', 'A', 'L', 'F', 'N', 'A', 'L', 'F', 'L', 'A', 'W', 'T', 'N', 'E', 'L', 'W', 'A', 'F', 'A', 'N', 'L', 'A', 'F', 'L', 'T', 'W', 'W', 'A', 'E', 'T', 'N', 'L', 'L', 'L', 'W', 'F', 'W', 'F', 'W', 'W', 'W', 'T', 'N', 'F', 'F', 'N', 'N', 'L', 'W', 'N', 'T', 'A', 'W', 'W', 'N', 'F', 'T', 'N', 'F', 'W', 'W', 'W', 'W', 'A', 'N', 'T', 'N', 'N', 'N', 'T', 'L', 'N', 'T', 'W', 'W', 'W', 'L', 'L', 'L', 'N', 'N', 'A', 'L', 'A', 'T', 'E', 'W', 'A', 'N', 'T', 'N', 'A', 'N', 'N', 'L', 'W', 'F', 'F', 'L', 'T', 'W', 'W', 'A', 'A', 'N', 'A', 'T', 'F', 'W', 'F', 'F', 'N', 'T', 'L', 'E', 'F', 'W', 'F', 'A', 'A', 'A', 'W', 'W', 'T', 'F']\n",
            "\n",
            "The length of y_test is: 196\n",
            "y_test:\n",
            "['F', 'N', 'W', 'A', 'T', 'L', 'W', 'W', 'N', 'A', 'E', 'A', 'F', 'T', 'W', 'T', 'E', 'E', 'W', 'W', 'L', 'A', 'F', 'E', 'W', 'N', 'A', 'T', 'W', 'L', 'W', 'N', 'F', 'W', 'N', 'W', 'L', 'W', 'A', 'N', 'F', 'N', 'W', 'L', 'L', 'N', 'A', 'A', 'N', 'F', 'N', 'L', 'E', 'L', 'N', 'L', 'A', 'N', 'N', 'L', 'A', 'T', 'A', 'W', 'T', 'W', 'L', 'L', 'W', 'A', 'T', 'A', 'T', 'F', 'T', 'W', 'A', 'L', 'W', 'T', 'E', 'A', 'W', 'N', 'F', 'E', 'L', 'F', 'N', 'W', 'E', 'W', 'W', 'F', 'T', 'W', 'W', 'W', 'F', 'A', 'N', 'W', 'A', 'N', 'T', 'A', 'W', 'T', 'L', 'E', 'L', 'W', 'L', 'E', 'F', 'N', 'E', 'E', 'E', 'W', 'A', 'T', 'F', 'F', 'A', 'F', 'E', 'W', 'T', 'F', 'E', 'W', 'E', 'W', 'W', 'A', 'L', 'T', 'T', 'F', 'F', 'T', 'E', 'E', 'L', 'W', 'F', 'F', 'A', 'F', 'N', 'L', 'W', 'T', 'W', 'N', 'E', 'E', 'F', 'L', 'N', 'W', 'L', 'W', 'L', 'E', 'A', 'L', 'N', 'F', 'E', 'F', 'F', 'L', 'W', 'L', 'N', 'A', 'W', 'A', 'W', 'W', 'A', 'T', 'L', 'L', 'A', 'T', 'L', 'E', 'T', 'T', 'E', 'L', 'W', 'L']\n",
            "\n",
            "\n",
            "MODEL: hybrid_byols\n",
            "\n",
            "The shape of X_train is: torch.Size([373, 2048])\n",
            "X_train:\n",
            "tensor([[-0.4688,  0.5001, -0.1864,  ..., -1.2090,  1.2901,  0.0915],\n",
            "        [ 0.4057, -1.0854, -0.4522,  ...,  0.8624,  3.0140,  0.2083],\n",
            "        [ 0.2208,  0.8972,  0.9757,  ...,  1.1843,  0.2234,  0.9068],\n",
            "        ...,\n",
            "        [ 1.5999,  0.1487,  0.3614,  ..., -0.0301, -0.6569, -0.7086],\n",
            "        [ 0.5872,  0.5702, -0.7913,  ..., -1.1178, -0.6894,  0.4957],\n",
            "        [ 0.9276, -1.4191, -1.9730,  ..., -0.2674, -0.0558, -0.2891]])\n",
            "\n",
            "The shape of X_test is: torch.Size([162, 2048])\n",
            "X_test:\n",
            "tensor([[ 1.3333, -0.1954, -0.0798,  ...,  0.1485, -0.6004, -0.3401],\n",
            "        [ 1.4871,  0.4246,  1.0183,  ..., -0.2140, -0.4822,  1.8459],\n",
            "        [ 0.4245,  0.0929,  0.7114,  ..., -0.6708, -1.0274,  0.3249],\n",
            "        ...,\n",
            "        [ 1.5060, -1.3033,  0.5395,  ..., -1.2667, -1.1189, -1.4268],\n",
            "        [-0.6898,  2.2488,  0.5864,  ...,  0.2461, -1.4752,  0.5615],\n",
            "        [ 1.3365,  1.4165,  1.2303,  ...,  1.2917, -1.3542, -0.6444]])\n",
            "\n",
            "The length of y_train is: 373\n",
            "y_train:\n",
            "['L', 'N', 'A', 'W', 'N', 'L', 'F', 'W', 'L', 'T', 'T', 'F', 'N', 'E', 'T', 'F', 'A', 'F', 'L', 'N', 'T', 'W', 'F', 'L', 'W', 'L', 'L', 'A', 'N', 'E', 'N', 'F', 'L', 'W', 'E', 'A', 'E', 'F', 'W', 'A', 'W', 'L', 'F', 'A', 'T', 'W', 'E', 'W', 'F', 'E', 'L', 'A', 'N', 'W', 'N', 'E', 'W', 'W', 'N', 'E', 'F', 'F', 'N', 'W', 'A', 'T', 'L', 'W', 'W', 'N', 'A', 'E', 'A', 'F', 'T', 'W', 'T', 'E', 'E', 'W', 'W', 'L', 'A', 'F', 'E', 'W', 'N', 'A', 'T', 'W', 'L', 'W', 'N', 'F', 'W', 'N', 'W', 'L', 'W', 'A', 'N', 'F', 'N', 'W', 'L', 'L', 'N', 'A', 'A', 'N', 'F', 'N', 'L', 'E', 'L', 'N', 'L', 'N', 'L', 'A', 'W', 'A', 'E', 'W', 'A', 'F', 'L', 'T', 'W', 'T', 'F', 'W', 'W', 'A', 'A', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'T', 'N', 'W', 'W', 'A', 'N', 'L', 'T', 'N', 'E', 'A', 'N', 'N', 'L', 'A', 'T', 'A', 'W', 'T', 'W', 'L', 'L', 'W', 'A', 'T', 'A', 'T', 'F', 'T', 'W', 'A', 'L', 'W', 'T', 'E', 'A', 'W', 'N', 'F', 'E', 'L', 'F', 'N', 'W', 'E', 'W', 'W', 'F', 'T', 'W', 'W', 'W', 'F', 'A', 'N', 'W', 'A', 'N', 'T', 'A', 'W', 'T', 'L', 'E', 'L', 'W', 'L', 'E', 'F', 'N', 'E', 'E', 'E', 'W', 'A', 'T', 'F', 'F', 'A', 'F', 'E', 'W', 'T', 'F', 'E', 'W', 'E', 'W', 'W', 'A', 'L', 'T', 'T', 'F', 'F', 'T', 'E', 'E', 'L', 'W', 'F', 'F', 'A', 'F', 'N', 'L', 'W', 'T', 'W', 'N', 'E', 'E', 'F', 'L', 'N', 'W', 'L', 'W', 'L', 'E', 'A', 'L', 'N', 'F', 'E', 'F', 'F', 'L', 'W', 'L', 'N', 'A', 'W', 'A', 'W', 'W', 'A', 'T', 'L', 'L', 'A', 'T', 'L', 'E', 'T', 'T', 'E', 'L', 'W', 'L', 'W', 'N', 'E', 'L', 'T', 'E', 'N', 'T', 'E', 'N', 'N', 'T', 'W', 'N', 'W', 'E', 'L', 'E', 'E', 'N', 'W', 'N', 'W', 'W', 'A', 'W', 'W', 'W', 'T', 'L', 'L', 'W', 'F', 'N', 'E', 'W', 'N', 'F', 'E', 'F', 'W', 'W', 'F', 'W', 'W', 'W', 'W', 'W', 'F', 'L', 'T', 'A', 'A', 'W', 'L', 'L', 'W', 'A', 'T', 'N', 'W', 'A', 'L', 'F', 'N', 'A', 'L', 'F', 'L', 'A', 'W', 'T', 'N', 'E', 'L', 'W', 'A', 'F', 'A', 'N', 'L']\n",
            "\n",
            "The length of y_test is: 162\n",
            "y_test:\n",
            "['L', 'A', 'W', 'A', 'L', 'W', 'N', 'T', 'T', 'N', 'L', 'F', 'T', 'A', 'T', 'F', 'L', 'F', 'W', 'N', 'T', 'N', 'N', 'N', 'A', 'W', 'F', 'F', 'T', 'T', 'W', 'W', 'F', 'L', 'L', 'L', 'L', 'F', 'W', 'A', 'W', 'L', 'F', 'L', 'T', 'T', 'F', 'N', 'F', 'N', 'F', 'W', 'N', 'W', 'W', 'W', 'A', 'N', 'A', 'F', 'L', 'T', 'W', 'W', 'A', 'E', 'T', 'N', 'L', 'L', 'L', 'W', 'F', 'W', 'F', 'W', 'W', 'W', 'T', 'N', 'F', 'F', 'N', 'N', 'L', 'W', 'N', 'T', 'A', 'W', 'W', 'N', 'F', 'T', 'N', 'F', 'W', 'W', 'W', 'W', 'A', 'N', 'T', 'N', 'N', 'N', 'T', 'L', 'N', 'T', 'W', 'W', 'W', 'L', 'L', 'L', 'N', 'N', 'A', 'L', 'A', 'T', 'E', 'W', 'A', 'N', 'T', 'N', 'A', 'N', 'N', 'L', 'W', 'F', 'F', 'L', 'T', 'W', 'W', 'A', 'A', 'N', 'A', 'T', 'F', 'W', 'F', 'F', 'N', 'T', 'L', 'E', 'F', 'W', 'F', 'A', 'A', 'A', 'W', 'W', 'T', 'F']\n",
            "\n",
            "\n",
            "MODEL: compare\n",
            "\n",
            "The shape of X_train is: torch.Size([393, 6373])\n",
            "X_train:\n",
            "tensor([[-0.1223,  1.1162, -0.9038,  ...,  1.8530,  0.0681,  0.4053],\n",
            "        [-0.1867, -1.1232, -0.9404,  ..., -0.3056, -0.4070, -0.9486],\n",
            "        [ 1.5298,  1.3942, -0.9778,  ..., -0.0971,  0.0873,  0.2846],\n",
            "        ...,\n",
            "        [ 0.3663,  0.2326, -1.2045,  ...,  0.9990,  1.1691,  0.2254],\n",
            "        [-0.7470, -0.0157,  0.6980,  ..., -1.0134, -1.2644, -1.4082],\n",
            "        [ 0.5729, -1.3782,  1.3255,  ...,  1.3074,  1.7894,  2.2911]])\n",
            "\n",
            "The shape of X_test is: torch.Size([142, 6373])\n",
            "X_test:\n",
            "tensor([[-0.9259, -0.1495, -0.8701,  ..., -0.7136, -1.5548, -0.7343],\n",
            "        [-0.3643, -0.2035, -0.8059,  ...,  0.3269, -0.5935, -0.5215],\n",
            "        [-0.5747,  1.6908,  1.4879,  ...,  0.4863, -0.6822, -0.3516],\n",
            "        ...,\n",
            "        [-0.7185,  0.4587, -0.1370,  ..., -0.7014,  0.0266, -0.5311],\n",
            "        [-0.2463, -1.0546, -0.9905,  ..., -1.1695, -1.0412, -0.7916],\n",
            "        [-1.1388, -1.3406,  1.4129,  ..., -1.3046,  0.0493, -0.4949]])\n",
            "\n",
            "The length of y_train is: 393\n",
            "y_train:\n",
            "['F', 'N', 'W', 'A', 'T', 'L', 'W', 'W', 'N', 'A', 'E', 'A', 'F', 'T', 'W', 'T', 'E', 'E', 'W', 'W', 'L', 'A', 'F', 'E', 'W', 'N', 'A', 'T', 'W', 'L', 'W', 'N', 'F', 'W', 'N', 'W', 'L', 'W', 'A', 'N', 'F', 'N', 'W', 'L', 'L', 'N', 'A', 'A', 'N', 'F', 'N', 'L', 'E', 'L', 'N', 'L', 'N', 'L', 'A', 'W', 'A', 'E', 'W', 'A', 'F', 'L', 'T', 'W', 'T', 'F', 'W', 'W', 'A', 'A', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'T', 'N', 'W', 'W', 'A', 'N', 'L', 'T', 'N', 'E', 'L', 'A', 'W', 'A', 'L', 'W', 'N', 'T', 'T', 'N', 'L', 'F', 'T', 'A', 'T', 'F', 'L', 'F', 'W', 'N', 'T', 'N', 'N', 'N', 'A', 'W', 'F', 'F', 'T', 'T', 'W', 'W', 'F', 'L', 'L', 'L', 'L', 'F', 'W', 'A', 'W', 'L', 'F', 'L', 'T', 'T', 'F', 'N', 'F', 'N', 'F', 'W', 'N', 'W', 'W', 'W', 'A', 'N', 'A', 'N', 'N', 'L', 'A', 'T', 'A', 'W', 'T', 'W', 'L', 'L', 'W', 'A', 'T', 'A', 'T', 'F', 'T', 'W', 'A', 'L', 'W', 'T', 'E', 'A', 'W', 'N', 'F', 'E', 'L', 'F', 'N', 'W', 'E', 'W', 'W', 'F', 'T', 'W', 'W', 'W', 'F', 'A', 'N', 'W', 'A', 'N', 'T', 'A', 'W', 'T', 'L', 'E', 'L', 'W', 'L', 'E', 'F', 'N', 'E', 'E', 'E', 'W', 'A', 'T', 'F', 'F', 'A', 'F', 'E', 'W', 'T', 'F', 'E', 'W', 'E', 'W', 'W', 'A', 'L', 'T', 'T', 'F', 'F', 'T', 'E', 'E', 'L', 'W', 'F', 'F', 'A', 'F', 'N', 'L', 'W', 'T', 'W', 'N', 'E', 'E', 'F', 'L', 'N', 'W', 'L', 'W', 'L', 'E', 'A', 'L', 'N', 'F', 'E', 'F', 'F', 'L', 'W', 'L', 'N', 'A', 'W', 'A', 'W', 'W', 'A', 'T', 'L', 'L', 'A', 'T', 'L', 'E', 'T', 'T', 'E', 'L', 'W', 'L', 'A', 'F', 'L', 'T', 'W', 'W', 'A', 'E', 'T', 'N', 'L', 'L', 'L', 'W', 'F', 'W', 'F', 'W', 'W', 'W', 'T', 'N', 'F', 'F', 'N', 'N', 'L', 'W', 'N', 'T', 'A', 'W', 'W', 'N', 'F', 'T', 'N', 'F', 'W', 'W', 'W', 'W', 'A', 'N', 'T', 'N', 'N', 'N', 'T', 'L', 'N', 'T', 'W', 'W', 'W', 'L', 'L', 'L', 'N', 'N', 'A', 'L', 'A', 'T', 'E', 'W', 'A', 'N', 'T', 'N', 'A', 'N', 'N', 'L', 'W', 'F', 'F', 'L', 'T', 'W', 'W', 'A', 'A', 'N', 'A', 'T', 'F', 'W', 'F', 'F', 'N', 'T', 'L', 'E', 'F', 'W', 'F', 'A', 'A', 'A', 'W', 'W', 'T', 'F']\n",
            "\n",
            "The length of y_test is: 142\n",
            "y_test:\n",
            "['L', 'N', 'A', 'W', 'N', 'L', 'F', 'W', 'L', 'T', 'T', 'F', 'N', 'E', 'T', 'F', 'A', 'F', 'L', 'N', 'T', 'W', 'F', 'L', 'W', 'L', 'L', 'A', 'N', 'E', 'N', 'F', 'L', 'W', 'E', 'A', 'E', 'F', 'W', 'A', 'W', 'L', 'F', 'A', 'T', 'W', 'E', 'W', 'F', 'E', 'L', 'A', 'N', 'W', 'N', 'E', 'W', 'W', 'N', 'E', 'F', 'W', 'N', 'E', 'L', 'T', 'E', 'N', 'T', 'E', 'N', 'N', 'T', 'W', 'N', 'W', 'E', 'L', 'E', 'E', 'N', 'W', 'N', 'W', 'W', 'A', 'W', 'W', 'W', 'T', 'L', 'L', 'W', 'F', 'N', 'E', 'W', 'N', 'F', 'E', 'F', 'W', 'W', 'F', 'W', 'W', 'W', 'W', 'W', 'F', 'L', 'T', 'A', 'A', 'W', 'L', 'L', 'W', 'A', 'T', 'N', 'W', 'A', 'L', 'F', 'N', 'A', 'L', 'F', 'L', 'A', 'W', 'T', 'N', 'E', 'L', 'W', 'A', 'F', 'A', 'N', 'L']\n",
            "\n",
            "\n",
            "MODEL: egemaps\n",
            "\n",
            "The shape of X_train is: torch.Size([419, 88])\n",
            "X_train:\n",
            "tensor([[-0.8863, -0.9552, -0.7365,  ..., -0.8289, -0.7315,  1.1737],\n",
            "        [-0.7762,  0.5082, -1.1051,  ..., -0.1428, -0.3711,  0.7715],\n",
            "        [ 0.1619, -1.1583,  0.4079,  ..., -0.6026, -0.2608,  0.2670],\n",
            "        ...,\n",
            "        [ 1.0881,  0.0717,  1.1756,  ..., -0.9356, -0.7277, -1.1197],\n",
            "        [-1.0704, -0.9794, -0.8689,  ...,  1.9520,  2.8624, -0.7409],\n",
            "        [ 1.9466, -0.1400,  2.0244,  ..., -0.4929, -0.5367, -0.4802]])\n",
            "\n",
            "The shape of X_test is: torch.Size([116, 88])\n",
            "X_test:\n",
            "tensor([[-0.7060, -0.1108, -0.8345,  ...,  0.2089, -0.0826, -0.7641],\n",
            "        [-0.4136,  0.4965, -0.3601,  ..., -0.1107, -0.4318, -0.2324],\n",
            "        [ 0.3285, -0.3333,  0.4266,  ..., -0.7804, -0.8025,  1.5211],\n",
            "        ...,\n",
            "        [ 1.2273, -0.9421,  1.4281,  ..., -0.2081,  0.7341,  0.5720],\n",
            "        [-1.0506, -0.1573, -0.9082,  ..., -0.1570, -0.8709,  1.2274],\n",
            "        [-1.0645,  0.5299, -1.1570,  ..., -0.8837, -0.9672,  0.2999]])\n",
            "\n",
            "The length of y_train is: 419\n",
            "y_train:\n",
            "['L', 'N', 'A', 'W', 'N', 'L', 'F', 'W', 'L', 'T', 'T', 'F', 'N', 'E', 'T', 'F', 'A', 'F', 'L', 'N', 'T', 'W', 'F', 'L', 'W', 'L', 'L', 'A', 'N', 'E', 'N', 'F', 'L', 'W', 'E', 'A', 'E', 'F', 'W', 'A', 'W', 'L', 'F', 'A', 'T', 'W', 'E', 'W', 'F', 'E', 'L', 'A', 'N', 'W', 'N', 'E', 'W', 'W', 'N', 'E', 'F', 'F', 'N', 'W', 'A', 'T', 'L', 'W', 'W', 'N', 'A', 'E', 'A', 'F', 'T', 'W', 'T', 'E', 'E', 'W', 'W', 'L', 'A', 'F', 'E', 'W', 'N', 'A', 'T', 'W', 'L', 'W', 'N', 'F', 'W', 'N', 'W', 'L', 'W', 'A', 'N', 'F', 'N', 'W', 'L', 'L', 'N', 'A', 'A', 'N', 'F', 'N', 'L', 'E', 'L', 'N', 'L', 'L', 'A', 'W', 'A', 'L', 'W', 'N', 'T', 'T', 'N', 'L', 'F', 'T', 'A', 'T', 'F', 'L', 'F', 'W', 'N', 'T', 'N', 'N', 'N', 'A', 'W', 'F', 'F', 'T', 'T', 'W', 'W', 'F', 'L', 'L', 'L', 'L', 'F', 'W', 'A', 'W', 'L', 'F', 'L', 'T', 'T', 'F', 'N', 'F', 'N', 'F', 'W', 'N', 'W', 'W', 'W', 'A', 'N', 'A', 'N', 'N', 'L', 'A', 'T', 'A', 'W', 'T', 'W', 'L', 'L', 'W', 'A', 'T', 'A', 'T', 'F', 'T', 'W', 'A', 'L', 'W', 'T', 'E', 'A', 'W', 'N', 'F', 'E', 'L', 'F', 'N', 'W', 'E', 'W', 'W', 'F', 'T', 'W', 'W', 'W', 'F', 'A', 'N', 'W', 'A', 'N', 'T', 'A', 'W', 'T', 'L', 'E', 'L', 'W', 'L', 'E', 'F', 'N', 'E', 'E', 'E', 'W', 'A', 'T', 'F', 'F', 'A', 'F', 'E', 'W', 'T', 'F', 'E', 'W', 'E', 'W', 'W', 'A', 'L', 'T', 'T', 'F', 'F', 'T', 'E', 'E', 'L', 'W', 'F', 'F', 'A', 'F', 'N', 'L', 'W', 'T', 'W', 'N', 'E', 'E', 'F', 'L', 'N', 'W', 'L', 'W', 'L', 'E', 'A', 'L', 'N', 'F', 'E', 'F', 'F', 'L', 'W', 'L', 'N', 'A', 'W', 'A', 'W', 'W', 'A', 'T', 'L', 'L', 'A', 'T', 'L', 'E', 'T', 'T', 'E', 'L', 'W', 'L', 'A', 'F', 'L', 'T', 'W', 'W', 'A', 'E', 'T', 'N', 'L', 'L', 'L', 'W', 'F', 'W', 'F', 'W', 'W', 'W', 'T', 'N', 'F', 'F', 'N', 'N', 'L', 'W', 'N', 'T', 'A', 'W', 'W', 'N', 'F', 'T', 'N', 'F', 'W', 'W', 'W', 'W', 'A', 'N', 'T', 'N', 'N', 'N', 'T', 'L', 'N', 'T', 'W', 'W', 'W', 'L', 'L', 'L', 'N', 'N', 'A', 'L', 'A', 'T', 'E', 'W', 'A', 'N', 'T', 'N', 'A', 'N', 'N', 'L', 'W', 'F', 'F', 'L', 'T', 'W', 'W', 'A', 'A', 'N', 'A', 'T', 'F', 'W', 'F', 'F', 'N', 'T', 'L', 'E', 'F', 'W', 'F', 'A', 'A', 'A', 'W', 'W', 'T', 'F']\n",
            "\n",
            "The length of y_test is: 116\n",
            "y_test:\n",
            "['N', 'L', 'A', 'W', 'A', 'E', 'W', 'A', 'F', 'L', 'T', 'W', 'T', 'F', 'W', 'W', 'A', 'A', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'T', 'N', 'W', 'W', 'A', 'N', 'L', 'T', 'N', 'E', 'W', 'N', 'E', 'L', 'T', 'E', 'N', 'T', 'E', 'N', 'N', 'T', 'W', 'N', 'W', 'E', 'L', 'E', 'E', 'N', 'W', 'N', 'W', 'W', 'A', 'W', 'W', 'W', 'T', 'L', 'L', 'W', 'F', 'N', 'E', 'W', 'N', 'F', 'E', 'F', 'W', 'W', 'F', 'W', 'W', 'W', 'W', 'W', 'F', 'L', 'T', 'A', 'A', 'W', 'L', 'L', 'W', 'A', 'T', 'N', 'W', 'A', 'L', 'F', 'N', 'A', 'L', 'F', 'L', 'A', 'W', 'T', 'N', 'E', 'L', 'W', 'A', 'F', 'A', 'N', 'L']\n"
          ]
        }
      ],
      "source": [
        "# # Phase_3: Train Test splitting\n",
        "\n",
        "# X_train_byols, X_test_byols, y_train_byols, y_test_byols = split_train_test(normalised_embeddings_byols, labels, speakers, test_size = 0.30)\n",
        "# X_train_compare, X_test_compare, y_train_compare, y_test_compare = split_train_test(normalised_embeddings_compare, labels, speakers, test_size = 0.30)\n",
        "# X_train_egemaps, X_test_egemaps, y_train_egemaps, y_test_egemaps = split_train_test(normalised_embeddings_egemaps, labels, speakers, test_size = 0.30)\n",
        "\n",
        "# X_trains = {'hybrid_byols':X_train_byols, 'compare':X_train_compare, 'egemaps':X_train_egemaps}\n",
        "# X_tests = {'hybrid_byols':X_test_byols, 'compare':X_test_compare, 'egemaps':X_test_egemaps}\n",
        "# y_trains = {'hybrid_byols':y_train_byols, 'compare':y_train_compare, 'egemaps':y_train_egemaps}\n",
        "# y_tests = {'hybrid_byols':y_test_byols, 'compare':y_test_compare, 'egemaps':y_test_egemaps}\n",
        "\n",
        "# # Verify\n",
        "# models = ['hybrid_byols', 'compare', 'egemaps']\n",
        "# for model in models:\n",
        "#   print()\n",
        "#   print()\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   print()\n",
        "#   print('The shape of X_train is: {}'.format(X_trains[model].shape))\n",
        "#   print('X_train:')\n",
        "#   print(X_trains[model])\n",
        "#   print()\n",
        "#   print('The shape of X_test is: {}'.format(X_tests[model].shape))\n",
        "#   print('X_test:')\n",
        "#   print(X_tests[model])\n",
        "#   print()\n",
        "#   print('The length of y_train is: {}'.format(len(y_trains[model])))\n",
        "#   print('y_train:')\n",
        "#   print(y_trains[model])\n",
        "#   print()\n",
        "#   print('The length of y_test is: {}'.format(len(y_tests[model])))\n",
        "#   print('y_test:')\n",
        "#   print(y_tests[model])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning functions"
      ],
      "metadata": {
        "id": "71fAXIdzcnWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLj0kbWmVZ3A"
      },
      "outputs": [],
      "source": [
        "# Defining a function for hyperparameter tuning and getting the accuracy on the test set\n",
        "\n",
        "def get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters):\n",
        "  '''\n",
        "  Splits into training and testing set with different speakers\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "  X_train: torch tensor\n",
        "    The normalised embeddings that will be used for training\n",
        "  X_test: torch tensor\n",
        "    The normalised embeddings that will be used for testing\n",
        "  y_train: list\n",
        "    The labels that will be used for training\n",
        "  y_test: list\n",
        "    The labels that will be used for testing\n",
        "  classifier: object\n",
        "    The instance of the classification model \n",
        "  parameters: dictionary\n",
        "    The dictionary of parameters for GridSearchCV \n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "    The dictionary of the best hyperparameters\n",
        "  \n",
        "  '''\n",
        "  grid = GridSearchCV(classifier, param_grid = parameters, cv=5, scoring='recall_macro')                     \n",
        "  grid.fit(X_train,y_train)\n",
        "  print('Accuracy :',grid.best_score_)\n",
        "  print('Best Parameters: {}'.format(grid.best_params_))\n",
        "  print('Accuracy on test_set: {}'.format(grid.score(X_test, y_test)))\n",
        "  return grid.score(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fckGF6LiFKVI"
      },
      "source": [
        "## Hyperparameter tuning and getting accuracy on EmoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RSIzZ9VKWvHv"
      },
      "outputs": [],
      "source": [
        "models = ['wav2vec2', 'hubert']\n",
        "\n",
        "# Logistic Regression\n",
        "print('Logistic Regression:')\n",
        "classifier = LogisticRegression()\n",
        "parameters = {'penalty' : ['l1','l2'], 'C': np.logspace(-4,2,7), 'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "print()\n",
        "\n",
        "# Support Vector Machine\n",
        "print('Support Vector Machine:')\n",
        "classifier = SVC()\n",
        "parameters = {'C': np.logspace(-2,3,6), 'gamma': np.logspace(-5,2,8), 'kernel':['rbf','poly','sigmoid','linear']}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "print()\n",
        "\n",
        "# Random Forest Classifier\n",
        "print('Random Forest Classifier:')\n",
        "classifier = RandomForestClassifier()\n",
        "parameters = {'n_estimators' : [50,100,200], 'max_features' : ['auto', 'log2', 'sqrt'], 'bootstrap' : [True, False]}\n",
        "for model in models:\n",
        "  print('MODEL: {}'.format(model))\n",
        "  get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models = ['hybrid_byols', 'compare', 'egemaps']\n",
        "\n",
        "# # Logistic Regression\n",
        "# print('Logistic Regression:')\n",
        "# classifier = LogisticRegression()\n",
        "# parameters = {'penalty' : ['l1','l2'], 'C': np.logspace(-4,2,7), 'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
        "# for model in models:\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "# print()\n",
        "\n",
        "# # Support Vector Machine\n",
        "# print('Support Vector Machine:')\n",
        "# classifier = SVC()\n",
        "# parameters = {'C': np.logspace(-2,3,6), 'gamma': np.logspace(-5,2,8), 'kernel':['rbf','poly','sigmoid','linear']}\n",
        "# for model in models:\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "# print()\n",
        "\n",
        "# # Random Forest Classifier\n",
        "# print('Random Forest Classifier:')\n",
        "# classifier = RandomForestClassifier()\n",
        "# parameters = {'n_estimators' : [50,100,200], 'max_features' : ['auto', 'log2', 'sqrt'], 'bootstrap' : [True, False]}\n",
        "# for model in models:\n",
        "#   print('MODEL: {}'.format(model))\n",
        "#   get_hyperparams(X_trains[model], X_tests[model], y_trains[model], y_tests[model], classifier, parameters)\n",
        "# print()\n"
      ],
      "metadata": {
        "id": "qxTGLq-045Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "Du0u4bUWIoz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# results = {'Logistic Regression': {'wav2vec2': 0, 'hubert': 0, 'hybrid_byols': 0, 'compare': 0, 'egemaps': 0},\n",
        "#         'Support Vector Machine': {'wav2vec2': 0, 'hubert': 0, 'hybrid_byols': 0, 'compare': 0, 'egemaps': 0},\n",
        "#         'Random Forest Classification': {'wav2vec2': 0, 'hubert': 0, 'hybrid_byols': 0, 'compare': 0, 'egemaps': 0}}"
      ],
      "metadata": {
        "id": "nWjiiUz2O62j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4TusdAY0rphr"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df = pd.DataFrame(results)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uFEQ32k6mJff"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SER_Demo-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFbOOoj1ln/nibXHMlQP/Z"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}