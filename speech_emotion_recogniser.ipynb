{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-dixit/speech_emotion_recognition/blob/main/speech_emotion_recogniser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyDk4cxk1wh"
      },
      "source": [
        "# Speech Emotion Recogniser\n",
        "\n",
        "A notebook to identify the emotion of an utterance in English. Trained on RAVDESS. The demo has been divided into 3 phases:\n",
        "- Phase 1: Uploading Audio File\n",
        "- Phase 2: Loading RAVDESS and extracting metadata\n",
        "- Phase 3: Speech Emotion Recognotion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About RAVDESS:\n",
        "- English\n",
        "- 7356 recordings\n",
        "- 24 actors (12 female, 12 male)\n",
        "- 8 emotions: neutral, calm, happy, sad, angry, fearful, surprise, and disgust\n",
        "\n",
        "### References:\n",
        "- Dataset: https://zenodo.org/record/1188976#.YvyPHexBy3K\n",
        "- Paper: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391\n",
        "\n",
        "\n",
        "Lets start by importing a uploading the audio clip.\n"
      ],
      "metadata": {
        "id": "pZ4u3Q5X47hN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48qMjEB-CLk"
      },
      "source": [
        "### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hed_5ILyX0ia",
        "outputId": "07e48db5-0fc0-4d3a-9d94-fc19e4ee663f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 40 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 51 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 61 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 71 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 75 kB 3.4 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!git clone -q https://github.com/GasserElbanna/serab-byols.git\n",
        "!python3 -m pip install -q -e ./serab-byols\n",
        "\n",
        "!pip install -q tqdm==4.60.0\n",
        "!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTY1RZ4UwCsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from random import sample\n",
        "from pathlib import Path \n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import serab_byols\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Uploading Audio File"
      ],
      "metadata": {
        "id": "ACxJ5Cwc_Wap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for recording audio"
      ],
      "metadata": {
        "id": "LiHfa4tQ_fGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ],
      "metadata": {
        "id": "tefc0KgZyTIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading audio"
      ],
      "metadata": {
        "id": "9ffUdEQ5_qyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio, sr = get_audio()\n",
        "audio = torch.Tensor(audio)\n",
        "test_list = [audio]\n",
        "print(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "iJ8QFDDfyZTG",
        "outputId": "ff377c9e-1c29-486d-ffd5-5d75e5217d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([  0.,   0.,   0.,  ..., -25., -21., -18.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6Yipn9MTLg"
      },
      "source": [
        "# Phase 2: Loading RAVDESS audio files and extracting metadata\n",
        "\n",
        "Includes downloading the dataset, loading audio files, resampling audio files, extracting metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a function for loading and resampling audio files"
      ],
      "metadata": {
        "id": "mcSr1Af5YzIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae5OWFUETeZC"
      },
      "outputs": [],
      "source": [
        "# Defining a function for loading and resampling audio files\n",
        "\n",
        "def load_audio_files(audio_files, resampling_frequency=16000, audio_list=None):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_files: string\n",
        "      The paths of the wav files \n",
        "  resampling_frequency: integer\n",
        "      The frequency which all audios will be resampled to\n",
        "  audio_list: list \n",
        "      The list of torch tensors of audios to which more audios need too be added, empty by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of torch tensors, one array for each audio file\n",
        "\n",
        "  '''\n",
        "  # Making audio_list\n",
        "  if audio_list is None:\n",
        "    audio_list = []\n",
        "\n",
        "  # Resampling\n",
        "  for audio in audio_files:\n",
        "    signal, fs = librosa.load(audio, sr=resampling_frequency)\n",
        "    audio_list.append(torch.from_numpy(signal))\n",
        "      \n",
        "  return audio_list\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata:\n",
        "Speakers: (24 speakers) \n",
        "- Odd numbered actors are male \n",
        "- Even numbered actors are female\n",
        "\n",
        "Labels: (8 labels)\n",
        "- 01 = neutral\n",
        "- 02 = calm\n",
        "- 03 = happy\n",
        "- 04 = sad\n",
        "- 05 = angry\n",
        "- 06 = fearful\n",
        "- 07 = disgust\n",
        "- 08 = surprised"
      ],
      "metadata": {
        "id": "zRYMAcdDB3J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and resampling audiofiles and collecting metadata on EmoDB dataset"
      ],
      "metadata": {
        "id": "PtuDXb9-OSZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Jncl071Q_z",
        "outputId": "8d931075-eff4-49dc-c667-04424cc4711f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files: 1440\n",
            "Number of speaker classes: 24\n",
            "Speaker classes: {'10', '17', '09', '18', '16', '02', '22', '05', '01', '12', '08', '06', '19', '21', '20', '23', '03', '04', '14', '11', '07', '24', '13', '15'}\n",
            "Number of speakers: 1440\n",
            "Number of label classes: 8\n",
            "Label classes: {'HAPPY', 'FEARFUL', 'SURPRISE', 'NEUTRAL', 'DISGUST', 'CALM', 'SAD', 'ANGRY'}\n",
            "Number of labels: 1440\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! wget -O ravdess-emotional-speech-audio.zip -q https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
        "! unzip -q ravdess-emotional-speech-audio.zip -d '/content/ravdess'\n",
        "\n",
        "# Select all the audio files\n",
        "audios = []\n",
        "for file in Path('/content/ravdess').glob(\"**/*.wav\"):\n",
        "    if not file.is_file(): \n",
        "        continue\n",
        "    audios.append(str(file))\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_list = load_audio_files(audios, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "old_labels = []\n",
        "for audio_file in audios:\n",
        "  file_name = audio_file.split('/')[4]\n",
        "  speakers.append(file_name[18:20])\n",
        "  old_labels.append(file_name[6:8])\n",
        "\n",
        "label_dict = {'01':'NEUTRAL', '02':'CALM', '03':'HAPPY', '04':'SAD', '05':'ANGRY', '06':'FEARFUL', '07':'DISGUST', '08':'SURPRISE'}\n",
        "labels = []\n",
        "for old_label in old_labels:\n",
        "  labels.append(label_dict[old_label])\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kPY4usMGbA"
      },
      "source": [
        "# Phase 3: Defining functions for Speech Emotion Recognition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3vG54u9D-r"
      },
      "source": [
        "### Audio embeddings extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCNer33gwaMl"
      },
      "outputs": [],
      "source": [
        "# Defining a function for generating audio embedding extraction models\n",
        "\n",
        "def audio_embeddings_model(model_name):\n",
        "  '''\n",
        "  Generates model for embedding extraction \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  mode_name: string\n",
        "      The model to used, could be 'hybrid_byols'\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  model: object\n",
        "      The embedding extraction model\n",
        "  '''\n",
        "  if model_name=='hybrid_byols':\n",
        "    model_name = 'cvt'\n",
        "    checkpoint_path = \"serab-byols/checkpoints/cvt_s1-d1-e64_s2-d1-e256_s3-d1-e512_BYOLAs64x96-osandbyolaloss6373-e100-bs256-lr0003-rs42.pth\"\n",
        "    model = serab_byols.load_model(checkpoint_path, model_name)\n",
        "  return model\n",
        "\n",
        "\n",
        "# Defining a function for embedding exctraction from the audio list\n",
        "def audio_embeddings(audio_list, model_name, model, sampling_rate=16000):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of arrays, one array for each audio file\n",
        "  model_name: string\n",
        "      The model to used, could be 'hybrid_byols'\n",
        "  model: object\n",
        "      The embedding extraction model generated by audio_embeddings_model function\n",
        "  sampling_rate: int\n",
        "      The sampling rate, 16 kHz by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  if model_name=='hybrid_byols':\n",
        "    embeddings_array = serab_byols.get_scene_embeddings(audio_list, model)\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91OM9gcMHWLN"
      },
      "source": [
        "### Speaker normalisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlFL7hMDwiqk"
      },
      "outputs": [],
      "source": [
        "# Defining a function for speaker normalisation using standard scaler\n",
        "\n",
        "def speaker_normalisation(embeddings_array, speakers):\n",
        "  '''\n",
        "  Normalises embeddings_array for each speaker\n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array of embeddings, one row for each audio file\n",
        "  speakers: list \n",
        "      The list of speakers\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  embeddings_array: array\n",
        "      The array containg normalised embeddings of all audio_files, dimension (number of audio files × n_feats)\n",
        "      \n",
        "  '''\n",
        "  speaker_ids = set(speakers)\n",
        "  for speaker_id in speaker_ids:\n",
        "    speaker_embeddings_indices = np.where(np.array(speakers)==speaker_id)[0]\n",
        "    speaker_embeddings = embeddings_array[speaker_embeddings_indices,:]\n",
        "    scaler = StandardScaler()\n",
        "    normalised_speaker_embeddings = scaler.fit_transform(speaker_embeddings)\n",
        "    embeddings_array[speaker_embeddings_indices] = torch.tensor(normalised_speaker_embeddings).float()\n",
        "  return embeddings_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning functions"
      ],
      "metadata": {
        "id": "71fAXIdzcnWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLj0kbWmVZ3A"
      },
      "outputs": [],
      "source": [
        "# Defining a function for hyperparameter tuning and getting the accuracy on the test set\n",
        "\n",
        "def get_hyperparams(X_train, X_test, y_train, classifier, parameters):\n",
        "  '''\n",
        "  Splits into training and testing set with different speakers\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "  X_train: torch tensor\n",
        "    The normalised embeddings that will be used for training\n",
        "  X_test: torch tensor\n",
        "    The normalised embeddings that will be used for testing\n",
        "  y_train: list\n",
        "    The labels that will be used for training\n",
        "  y_test: list\n",
        "    The labels that will be used for testing\n",
        "  classifier: object\n",
        "    The instance of the classification model \n",
        "  parameters: dictionary\n",
        "    The dictionary of parameters for GridSearchCV \n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "    The dictionary of the best hyperparameters\n",
        "  \n",
        "  '''\n",
        "  grid = GridSearchCV(classifier, param_grid = parameters, cv=5, scoring='recall_macro')                     \n",
        "  grid.fit(X_train,y_train)\n",
        "  print('recall_macro :',grid.best_score_)\n",
        "  print('Best Parameters: {}'.format(grid.best_params_))\n",
        "  prediction = grid.predict(X_test)\n",
        "  print('PREDICTION: {}'.format(prediction))\n",
        "  return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fckGF6LiFKVI"
      },
      "source": [
        "### Pipeline function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function for all steps \n",
        "\n",
        "def pipeline(audio_list, test_list, speakers, labels):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_files: string\n",
        "      The paths of the wav files \n",
        "  resampling_frequency: integer\n",
        "      The frequency which all audios will be resampled to\n",
        "  audio_list: list \n",
        "      The list of torch tensors of audios to which more audios need too be added, empty by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of torch tensors, one array for each audio file\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Embeddings Extraction\n",
        "  model = audio_embeddings_model(model_name = 'hybrid_byols')\n",
        "  embeddings_array = audio_embeddings(audio_list, model_name = 'hybrid_byols', model=model)\n",
        "  test_embeddings_array = audio_embeddings(test_list, model_name = 'hybrid_byols', model=model)\n",
        "  print('embeddings_array shape: {}'.format(embeddings_array.shape))\n",
        "  print('test_embeddings_array shape: {}'.format(test_embeddings_array.shape))\n",
        "\n",
        "  # Speaker Normalisation\n",
        "  normalised_embeddings = speaker_normalisation(embeddings_array, speakers)\n",
        "  print('normalised_embeddings shape: {}'.format(normalised_embeddings.shape))\n",
        "  columnwise_mean = torch.mean(normalised_embeddings, 0)\n",
        "  if torch.all(columnwise_mean < 10**(-6)):\n",
        "    print('PASSED: All means are less than 10**-6')\n",
        "  else:\n",
        "    print('FAILED: All means are NOT less than 10**-6')\n",
        "\n",
        "  X_train = normalised_embeddings\n",
        "  y_train = labels\n",
        "  X_test = test_embeddings_array\n",
        "\n",
        "  # Getting hyperparameters and checking max_recall\n",
        "  print('Support Vector Machine:')\n",
        "  classifier = SVC()\n",
        "  parameters = {'C': np.logspace(-2,4,7), 'gamma': np.logspace(-5,-3,9), 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "  get_hyperparams(X_train, X_test, y_train, classifier, parameters)\n",
        "  "
      ],
      "metadata": {
        "id": "bHQJ_qRRFY4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "Getting the emotion of the audio based on a model trained using Hybrid BYOL-S on RAVDESS"
      ],
      "metadata": {
        "id": "Du0u4bUWIoz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TusdAY0rphr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfe337d-64ed-4fcd-9967-3444880c7a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings...: 100%|██████████| 1440/1440 [01:33<00:00, 15.39it/s]\n",
            "Generating Embeddings...: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings_array shape: torch.Size([1440, 2048])\n",
            "test_embeddings_array shape: torch.Size([1, 2048])\n",
            "normalised_embeddings shape: torch.Size([1440, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "Support Vector Machine:\n",
            "recall_macro : 0.7818572874493926\n",
            "Best Parameters: {'C': 10.0, 'gamma': 0.00031622776601683794, 'kernel': 'rbf'}\n",
            "PREDICTION: ['FEARFUL']\n"
          ]
        }
      ],
      "source": [
        "pipeline(audio_list, test_list, speakers, labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJgxonzSFrS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B48qMjEB-CLk",
        "ACxJ5Cwc_Wap",
        "mcSr1Af5YzIH",
        "PtuDXb9-OSZU",
        "aP3vG54u9D-r",
        "91OM9gcMHWLN",
        "71fAXIdzcnWP",
        "fckGF6LiFKVI"
      ],
      "name": "speech_emotion_recogniser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrtpfh4R+/dblyXwJ19Yt0",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}