{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyDk4cxk1wh"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48qMjEB-CLk"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hed_5ILyX0ia",
        "outputId": "9576af1a-d29f-4dfb-a4fa-7119c871ad32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-0.5.12-py3-none-any.whl (496 kB)\n",
            "\u001b[K     |████████████████████████████████| 496 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from speechbrain) (4.64.0)\n",
            "Collecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.1.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from speechbrain) (0.12.0+cu113)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.21.6)\n",
            "Collecting torch<=1.11,>=1.7\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.11,>=1.7->speechbrain) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->speechbrain) (3.0.9)\n",
            "Collecting ruamel.yaml>=0.17.8\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->speechbrain) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (1.24.3)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 32.6 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 26.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hyperpyyaml\n",
            "  Building wheel for hyperpyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyperpyyaml: filename=HyperPyYAML-1.0.1-py3-none-any.whl size=15192 sha256=b368ee23dc3567fd2a8206fcb8aa404e59263ee73e30021e8762ffe181cb6dff\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/87/65/266d722c3932f81f16332ce842e972be8421e3a9cd3771766b\n",
            "Successfully built hyperpyyaml\n",
            "Installing collected packages: ruamel.yaml.clib, torch, ruamel.yaml, pyyaml, torchaudio, sentencepiece, hyperpyyaml, huggingface-hub, speechbrain\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.0+cu113\n",
            "    Uninstalling torchaudio-0.12.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.8.1 hyperpyyaml-1.0.1 pyyaml-6.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sentencepiece-0.1.96 speechbrain-0.5.12 torch-1.11.0 torchaudio-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.21.0\n",
            "Cloning into 'serab-byols'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 250 (delta 10), reused 16 (delta 6), pack-reused 228\u001b[K\n",
            "Receiving objects: 100% (250/250), 112.55 MiB | 33.50 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Checking out files: 100% (35/35), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/serab-byols\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.8.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.51.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.11.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (4.64.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.9)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from serab-byols==0.0.0) (1.0.1)\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (0.3.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (0.10.3.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->serab-byols==0.0.0) (2.1.9)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->serab-byols==0.0.0) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->serab-byols==0.0.0) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa->serab-byols==0.0.0) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->serab-byols==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->serab-byols==0.0.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->serab-byols==0.0.0) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->serab-byols==0.0.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa->serab-byols==0.0.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->serab-byols==0.0.0) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->serab-byols==0.0.0) (4.1.1)\n",
            "Installing collected packages: einops, serab-byols\n",
            "  Running setup.py develop for serab-byols\n",
            "Successfully installed einops-0.4.1 serab-byols-0.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tqdm==4.60.0\n",
            "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tqdm-4.60.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.4.1-py3-none-any.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting audinterface>=0.7.0\n",
            "  Downloading audinterface-0.9.1-py3-none-any.whl (30 kB)\n",
            "Collecting audobject>=0.6.1\n",
            "  Downloading audobject-0.7.5-py3-none-any.whl (24 kB)\n",
            "Collecting audformat<2.0.0,>=0.12.1\n",
            "  Downloading audformat-0.14.3-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting audresample<2.0.0,>=1.1.0\n",
            "  Downloading audresample-1.1.0-py3-none-any.whl (635 kB)\n",
            "\u001b[K     |████████████████████████████████| 635 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting iso3166\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Collecting oyaml\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Collecting audeer<2.0.0,>=1.18.0\n",
            "  Downloading audeer-1.18.0-py3-none-any.whl (20 kB)\n",
            "Collecting audiofile>=0.4.0\n",
            "  Downloading audiofile-1.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.3.5)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (6.0)\n",
            "Collecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from audeer<2.0.0,>=1.18.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (4.60.0)\n",
            "Collecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from audobject>=0.6.1->opensmile) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.21)\n",
            "Building wheels for collected packages: iso-639\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=a1d9f80636b0061d752290b8ab3d1526b61ac52acf4d20ea04fb32b08242738b\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "Successfully built iso-639\n",
            "Installing collected packages: sox, audeer, oyaml, iso3166, iso-639, audiofile, audresample, audformat, audobject, audinterface, opensmile\n",
            "Successfully installed audeer-1.18.0 audformat-0.14.3 audinterface-0.9.1 audiofile-1.1.0 audobject-0.7.5 audresample-1.1.0 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.4.1 oyaml-1.0 sox-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain\n",
        "!pip install transformers\n",
        "!git clone https://github.com/GasserElbanna/serab-byols.git\n",
        "!python3 -m pip install -e ./serab-byols\n",
        "\n",
        "!pip install tqdm==4.60.0\n",
        "!pip install opensmile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "2r74_jiSWxp9",
        "outputId": "50c72466-11a0-4949-d81c-b89bd6537e60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bad7f64f-cf22-49e9-96ec-6dd30e2d8f6a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bad7f64f-cf22-49e9-96ec-6dd30e2d8f6a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-09d9d833-a1ee-4bad-ad34-b8bf88a65393\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-09d9d833-a1ee-4bad-ad34-b8bf88a65393\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving utilities.py to utilities.py\n"
          ]
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "files.upload()\n",
        "\n",
        "# Name directory\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTY1RZ4UwCsG",
        "outputId": "342cd577-9445-4adb-9894-3cb2462a3fdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from random import sample\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import opensmile\n",
        "import serab_byols\n",
        "from transformers import Wav2Vec2Model, HubertModel\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from utilities import load_audio_files, audio_embeddings_model, audio_embeddings, speaker_normalisation, split_train_test, get_hyperparams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjn_XyzhFIr7"
      },
      "source": [
        "# Defining a function for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvmZ0RwTDXCY"
      },
      "outputs": [],
      "source": [
        "# Defining a function for all steps \n",
        "\n",
        "def pipeline(audio_list, speakers, labels, model_names, dataset = None):\n",
        "  '''\n",
        "  Loads and resamples audio files \n",
        "  \n",
        "  Parameters\n",
        "  ------------\n",
        "  audio_files: string\n",
        "      The paths of the wav files \n",
        "  resampling_frequency: integer\n",
        "      The frequency which all audios will be resampled to\n",
        "  audio_list: list \n",
        "      The list of torch tensors of audios to which more audios need too be added, empty by default\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  audio_list: list\n",
        "      A list of torch tensors, one array for each audio file\n",
        "\n",
        "  '''\n",
        "  for model_name in model_names:\n",
        "    print('MODEL: {}'.format(model_name))\n",
        "\n",
        "    # Embeddings Extraction\n",
        "    model = audio_embeddings_model(model_name = model_name)\n",
        "    embeddings_array = audio_embeddings(audio_list, model_name=model_name, model=model)\n",
        "    print('embeddings_array shape: {}'.format(embeddings_array.shape))\n",
        "\n",
        "    # Speaker Normalisation\n",
        "    normalised_embeddings = speaker_normalisation(embeddings_array, speakers)\n",
        "    print('normalised_embeddings shape: {}'.format(normalised_embeddings.shape))\n",
        "    columnwise_mean = torch.mean(normalised_embeddings, 0)\n",
        "    if torch.all(columnwise_mean < 10**(-6)):\n",
        "      print('PASSED: All means are less than 10**-6')\n",
        "    else:\n",
        "      print('FAILED: All means are NOT less than 10**-6')\n",
        "\n",
        "    # Train Test Splitting\n",
        "    X_train, X_test, y_train, y_test = split_train_test(normalised_embeddings, labels, speakers, test_size = 0.30)\n",
        "    print('X_train shape: {}'.format(X_train.shape))\n",
        "    print('X_test shape: {}'.format(X_test.shape))\n",
        "    print('y_train len: {}'.format(len(y_train)))\n",
        "    print('y_test len: {}'.format(len(y_test)))\n",
        "    print()\n",
        "\n",
        "    # Getting hyperparameters and checking Accuracy\n",
        "    print('Logistic Regression:')\n",
        "    classifier = LogisticRegression()\n",
        "    parameters = {'penalty' : ['l1','l2'], 'C': np.logspace(-3,2,6), 'solver': ['lbfgs', 'sag']}\n",
        "    get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters)\n",
        "    print('Support Vector Machine:')\n",
        "    classifier = SVC()\n",
        "    parameters = {'C': np.logspace(-2,4,4), 'gamma': np.logspace(-5,3,5), 'kernel':['rbf']}\n",
        "    get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters)\n",
        "    print('Random Forest Classifier:')\n",
        "    classifier = RandomForestClassifier()\n",
        "    parameters = {'n_estimators' : [50,100,200], 'max_features' : ['auto', 'log2', 'sqrt'], 'bootstrap' : [False]}\n",
        "    get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters)\n",
        "    print()\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I0sxA2JRzsC"
      },
      "source": [
        "# Dataset: Canadian French Emotion (CaFE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "433AGNvofvBz",
        "outputId": "f7338e71-a61e-41af-afba-9668899c0036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-31 15:53:44--  https://zenodo.org/record/1478765/files/CaFE_48k.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 187220691 (179M) [application/octet-stream]\n",
            "Saving to: ‘CaFE_48k.zip?download=1’\n",
            "\n",
            "CaFE_48k.zip?downlo 100%[===================>] 178.55M  25.6MB/s    in 8.0s    \n",
            "\n",
            "2022-07-31 15:53:54 (22.3 MB/s) - ‘CaFE_48k.zip?download=1’ saved [187220691/187220691]\n",
            "\n",
            "Number of audio files: 936\n",
            "Number of speaker classes: 6\n",
            "Speaker classes: {'6', '3', '2', '5', '1', '4'}\n",
            "Number of speakers: 936\n",
            "Number of label classes: 7\n",
            "Label classes: {'Neutre', 'Surprise', 'Joie', 'Peur', 'DВgoЦt', 'ColКre', 'Tristesse'}\n",
            "Number of labels: 936\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! wget https://zenodo.org/record/1478765/files/CaFE_48k.zip?download=1\n",
        "! unzip -q CaFE_48k.zip?download=1 -d /content/cafe\n",
        "\n",
        "# Select all the audio files\n",
        "audios = []\n",
        "for file in Path('/content/cafe').glob(\"**/*.wav\"):\n",
        "    if not file.is_file(): \n",
        "        continue\n",
        "    audios.append(str(file))\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_list = load_audio_files(audios, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audios:\n",
        "  file_name = audio_file.split('/')[-1]\n",
        "  speakers.append(file_name.split('-')[-1].split('.')[0])\n",
        "  labels.append(audio_file.split('/')[3])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X64mO8pifvB0"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw1lcNVlfvB0"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'CaFE')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo63o7iBfvB0",
        "outputId": "3ae79669-b933-44d2-edef-48e41f7500f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 936/936 [01:16<00:00, 12.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([936, 2048])\n",
            "normalised_embeddings shape: torch.Size([936, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([780, 2048])\n",
            "X_test shape: torch.Size([156, 2048])\n",
            "y_train len: 780\n",
            "y_test len: 156\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.780952380952381\n",
            "Best Parameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.8452380952380952\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.7904761904761906\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.8511904761904763\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.7178571428571429\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.7857142857142857\n",
            "\n",
            "\n",
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 936/936 [02:14<00:00,  6.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([936, 6373])\n",
            "normalised_embeddings shape: torch.Size([936, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([780, 6373])\n",
            "X_test shape: torch.Size([156, 6373])\n",
            "y_train len: 780\n",
            "y_test len: 156\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6702380952380953\n",
            "Best Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.6547619047619048\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6452380952380953\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6369047619047619\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6011904761904763\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.6071428571428571\n",
            "\n",
            "\n",
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 936/936 [02:36<00:00,  6.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([936, 88])\n",
            "normalised_embeddings shape: torch.Size([936, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([780, 88])\n",
            "X_test shape: torch.Size([156, 88])\n",
            "y_train len: 780\n",
            "y_test len: 156\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.5916666666666666\n",
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.5833333333333334\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.5880952380952381\n",
            "Best Parameters: {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.5595238095238095\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6654761904761904\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'log2', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.5773809523809524\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols', 'compare', 'egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'CaFE')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnkwbFe5GgRT"
      },
      "source": [
        "# Dataset: Persian Speech Emotion Detection Dataset (ShEMO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLh5Gtma3YrD",
        "outputId": "0e205267-61ce-4cb6-ff01-7d9d079843f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading shemo-persian-speech-emotion-detection-database.zip to /content\n",
            " 99% 824M/829M [00:06<00:00, 166MB/s]\n",
            "100% 829M/829M [00:06<00:00, 131MB/s]\n",
            "Number of audio files: 3000\n",
            "Number of speaker classes: 87\n",
            "Speaker classes: {'44', '23', '16', '38', '86', '15', '68', '72', '11', '09', '84', '51', '17', '80', '41', '79', '45', '08', '43', '67', '36', '76', '34', '06', '75', '30', '70', '31', '58', '32', '74', '28', '33', '56', '64', '62', '66', '59', '78', '53', '18', '55', '49', '73', '04', '81', '69', '27', '46', '40', '48', '42', '10', '20', '26', '12', '82', '07', '87', '37', '24', '47', '39', '54', '57', '13', '77', '19', '01', '52', '85', '60', '22', '65', '63', '21', '25', '71', '14', '50', '61', '05', '03', '29', '83', '35', '02'}\n",
            "Number of speakers: 3000\n",
            "Number of label classes: 6\n",
            "Label classes: {'W', 'S', 'H', 'A', 'N', 'F'}\n",
            "Number of labels: 3000\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -d mansourehk/shemo-persian-speech-emotion-detection-database\n",
        "! unzip -q shemo-persian-speech-emotion-detection-database.zip -d shemo;\n",
        "\n",
        "# Select all the audio files\n",
        "audios = []\n",
        "for file in Path('/content/shemo').glob(\"**/*.wav\"):\n",
        "    if not file.is_file(): \n",
        "        continue\n",
        "    audios.append(str(file))\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_list = load_audio_files(audios, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audios:\n",
        "  file_name = audio_file.split('/')[4]\n",
        "  speakers.append(file_name[4:6])\n",
        "  labels.append(file_name[3])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciOZGXJ9Y2GL"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTw0pdEjY2GM"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'ShEMO')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzJRvk5gY2GM",
        "outputId": "aa4fcd05-a69d-4ded-b0ee-483aca7f4964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 3000/3000 [04:11<00:00, 11.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([3000, 2048])\n",
            "normalised_embeddings shape: torch.Size([3000, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1626, 2048])\n",
            "X_test shape: torch.Size([1374, 2048])\n",
            "y_train len: 1626\n",
            "y_test len: 1374\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6241436715715076\n",
            "Best Parameters: {'C': 10.0, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.5406500133083051\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6103820788523637\n",
            "Best Parameters: {'C': 10000.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.5088360425600119\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.48096135022230657\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.3835617820821429\n",
            "\n",
            "\n",
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [07:00<00:00,  7.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([3000, 6373])\n",
            "normalised_embeddings shape: torch.Size([3000, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1979, 6373])\n",
            "X_test shape: torch.Size([1021, 6373])\n",
            "y_train len: 1979\n",
            "y_test len: 1021\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.5428248104221408\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.6126867592495242\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.515592372361884\n",
            "Best Parameters: {'C': 10000.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6144201896699778\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.4166988058224397\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 50}\n",
            "Accuracy on test_set: 0.4379319816774401\n",
            "\n",
            "\n",
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [08:09<00:00,  6.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([3000, 88])\n",
            "normalised_embeddings shape: torch.Size([3000, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1977, 88])\n",
            "X_test shape: torch.Size([1023, 88])\n",
            "y_train len: 1977\n",
            "y_test len: 1023\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.4686051551668714\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.5025268582186633\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.4704030410043183\n",
            "Best Parameters: {'C': 10000.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.45075655053032015\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.42650405787906076\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.4300003123614043\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols', 'compare', 'egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'ShEMO')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXMPk6AG7O-"
      },
      "source": [
        "# Dataset: EmoDB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Jncl071Q_z",
        "outputId": "830107a1-c412-4aca-b21e-9aa5254feb44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading berlin-database-of-emotional-speech-emodb.zip to /content\n",
            " 63% 24.0M/38.0M [00:00<00:00, 130MB/s] \n",
            "100% 38.0M/38.0M [00:00<00:00, 149MB/s]\n",
            "Number of audio files: 535\n",
            "Number of speaker classes: 10\n",
            "Speaker classes: {3, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n",
            "Number of speakers: 535\n",
            "Number of label classes: 7\n",
            "Label classes: {'T', 'W', 'L', 'A', 'E', 'N', 'F'}\n",
            "Number of labels: 535\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -d piyushagni5/berlin-database-of-emotional-speech-emodb\n",
        "! unzip -q berlin-database-of-emotional-speech-emodb.zip\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_files = glob(os.path.join('/content/wav','*.wav'))\n",
        "audio_list= load_audio_files(audio_files, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audio_files:\n",
        "  file_name = audio_file.split('/')[3]\n",
        "  speakers.append(int(file_name[:2]))\n",
        "  labels.append(file_name[5:6])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTy9hdXlhSI7"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjzPRinmhSJI"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'EmoDB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZGcj5eVhSJJ",
        "outputId": "0164c9f9-9f8a-499f-b034-386590049a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 535/535 [00:28<00:00, 18.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([535, 2048])\n",
            "normalised_embeddings shape: torch.Size([535, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([398, 2048])\n",
            "X_test shape: torch.Size([137, 2048])\n",
            "y_train len: 398\n",
            "y_test len: 137\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.8763512677798394\n",
            "Best Parameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.9182839802587701\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.876091527520099\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.8915566226490597\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.7946661024856514\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.7447827615894843\n",
            "\n",
            "\n",
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 535/535 [00:59<00:00,  9.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([535, 6373])\n",
            "normalised_embeddings shape: torch.Size([535, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([387, 6373])\n",
            "X_test shape: torch.Size([148, 6373])\n",
            "y_train len: 387\n",
            "y_test len: 148\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.8846693082827537\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.8397108471180035\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.8763696714116882\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.8530255278289977\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.8092630991790655\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.7786581381932427\n",
            "\n",
            "\n",
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 535/535 [01:02<00:00,  8.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([535, 88])\n",
            "normalised_embeddings shape: torch.Size([535, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([381, 88])\n",
            "X_test shape: torch.Size([154, 88])\n",
            "y_train len: 381\n",
            "y_test len: 154\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.8160259414770692\n",
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.7805394605394606\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.7850852781303909\n",
            "Best Parameters: {'C': 10000.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.7710089910089909\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.784940706745218\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.7266067266067265\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols', 'compare', 'egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'EmoDB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRzA2MjjHi4D"
      },
      "source": [
        "# Dataset: RAVDESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnP07Wkg-1ls",
        "outputId": "fb0aa2cd-cd7f-4deb-cac9-18c76297c663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ravdess-emotional-speech-audio.zip to /content\n",
            " 96% 414M/429M [00:02<00:00, 159MB/s]\n",
            "100% 429M/429M [00:02<00:00, 165MB/s]\n",
            "Number of audio files: 1440\n",
            "Number of speaker classes: 24\n",
            "Speaker classes: {'10', '22', '20', '06', '12', '23', '07', '16', '24', '21', '15', '13', '19', '11', '14', '18', '01', '05', '09', '03', '04', '17', '08', '02'}\n",
            "Number of speakers: 1440\n",
            "Number of label classes: 8\n",
            "Label classes: {'05', '07', '03', '04', '06', '08', '01', '02'}\n",
            "Number of labels: 1440\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n",
        "! unzip -q ravdess-emotional-speech-audio.zip -d '/content/ravdess'\n",
        "\n",
        "# Select all the audio files\n",
        "audios = []\n",
        "for file in Path('/content/ravdess/audio_speech_actors_01-24').glob(\"**/*.wav\"):\n",
        "    if not file.is_file(): \n",
        "        continue\n",
        "    audios.append(str(file))\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_list = load_audio_files(audios, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audios:\n",
        "  file_name = audio_file.split('/')[5]\n",
        "  speakers.append(file_name[18:20])\n",
        "  labels.append(file_name[6:8])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUSeKS_hXbh"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj73pho9hXbh"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'RAVDESS')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "al6DnO6BhXbi",
        "outputId": "0558c701-ac11-422b-9c51-eb2374d4c41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 1440/1440 [01:36<00:00, 14.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([1440, 2048])\n",
            "normalised_embeddings shape: torch.Size([1440, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1020, 2048])\n",
            "X_test shape: torch.Size([420, 2048])\n",
            "y_train len: 1020\n",
            "y_test len: 420\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.8034035409035409\n",
            "Best Parameters: {'C': 10.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.7544642857142857\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.8018594831094831\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.7678571428571429\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6595924908424908\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.6875\n",
            "\n",
            "\n",
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1440/1440 [02:59<00:00,  8.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([1440, 6373])\n",
            "normalised_embeddings shape: torch.Size([1440, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1020, 6373])\n",
            "X_test shape: torch.Size([420, 6373])\n",
            "y_train len: 1020\n",
            "y_test len: 420\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6754070004070003\n",
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.6629464285714286\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6596891534391534\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6495535714285714\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.5954772079772079\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.5825892857142858\n",
            "\n",
            "\n",
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1440/1440 [03:22<00:00,  7.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([1440, 88])\n",
            "normalised_embeddings shape: torch.Size([1440, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([1020, 88])\n",
            "X_test shape: torch.Size([420, 88])\n",
            "y_train len: 1020\n",
            "y_test len: 420\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6084554334554334\n",
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.5602678571428571\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.5886345136345137\n",
            "Best Parameters: {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.5424107142857143\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.5794210419210419\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.53125\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols', 'compare', 'egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'RAVDESS')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGS0iS0IO4i"
      },
      "source": [
        "# Dataset: Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvww89A5fv6n",
        "outputId": "1775da0f-3640-4df5-a200-0fb3206cf6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cremad.zip to /content\n",
            "100% 451M/451M [00:18<00:00, 23.4MB/s]\n",
            "100% 451M/451M [00:18<00:00, 25.6MB/s]\n",
            "Number of audio files: 7442\n",
            "Number of speaker classes: 91\n",
            "Speaker classes: {1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023}\n",
            "Number of speakers: 7442\n",
            "Number of label classes: 6\n",
            "Label classes: {'NEU', 'ANG', 'DIS', 'FEA', 'SAD', 'HAP'}\n",
            "Number of labels: 7442\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -d ejlok1/cremad\n",
        "! unzip -q cremad.zip\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_files = glob(os.path.join('/content/AudioWAV','*.wav'))\n",
        "audio_list = load_audio_files(audio_files, resampling_frequency=16000)\n",
        "\n",
        "# Make speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audio_files:\n",
        "  file_name = audio_file.split('/')[3]\n",
        "  speakers.append(int(file_name[:4]))\n",
        "  labels.append(file_name[9:12])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX4u7d3hhY00"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs5Q_SGDhY00"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'CREMA-D')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9yA1cdehY00",
        "outputId": "5a614ff9-f680-487f-b58b-16f23e45ed90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 7442/7442 [05:44<00:00, 21.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([7442, 2048])\n",
            "normalised_embeddings shape: torch.Size([7442, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([5235, 2048])\n",
            "X_test shape: torch.Size([2207, 2048])\n",
            "y_train len: 5235\n",
            "y_test len: 2207\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.7349894609501295\n",
            "Best Parameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.7621230593494462\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.7175696211152425\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.7275318384763662\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6261708205415812\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.6505112828201284\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'CREMA-D')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64H062h0ck_4",
        "outputId": "41d80e98-ad8f-489f-f7df-480f22388016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7442/7442 [12:35<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([7442, 88])\n",
            "normalised_embeddings shape: torch.Size([7442, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([5241, 88])\n",
            "X_test shape: torch.Size([2201, 88])\n",
            "y_train len: 5241\n",
            "y_test len: 2201\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6142979092206828\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.6009934049181415\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6245229551670562\n",
            "Best Parameters: {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6086048142992864\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.5926097429547971\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'log2', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.5664189368330351\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'CREMA-D')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvWWHu0bcm-Y",
        "outputId": "72cb9628-464d-47e9-ccab-22279ea3466f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7442/7442 [12:13<00:00, 10.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([7442, 6373])\n",
            "normalised_embeddings shape: torch.Size([7442, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([5235, 6373])\n",
            "X_test shape: torch.Size([2207, 6373])\n",
            "y_train len: 5235\n",
            "y_test len: 2207\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6805116194853924\n",
            "Best Parameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.71636159942007\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6576467155676393\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6872044746857341\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6136913013623977\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.632019155257536\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['compare']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'CREMA-D')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WJP5oglHV1g"
      },
      "source": [
        "# Dataset: SAVEE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVxAV4Xy28bw",
        "outputId": "8e39c726-98a9-497b-eee6-0ea82ce7db59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading savee-database.zip to /content\n",
            " 90% 193M/215M [00:07<00:00, 24.1MB/s]\n",
            "100% 215M/215M [00:07<00:00, 28.3MB/s]\n",
            "Number of audio files: 480\n",
            "Number of speaker classes: 4\n",
            "Speaker classes: {'DC', 'JK', 'KL', 'JE'}\n",
            "Number of speakers: 480\n",
            "Number of label classes: 6\n",
            "Label classes: {'h', 's', 'n', 'd', 'a', 'f'}\n",
            "Number of labels: 480\n"
          ]
        }
      ],
      "source": [
        "# Phase_1\n",
        "# Load dataset\n",
        "! kaggle datasets download -d barelydedicated/savee-database\n",
        "! unzip -q savee-database.zip \n",
        "\n",
        "# Select all the audio files\n",
        "audios = []\n",
        "for file in Path('/content/AudioData').glob(\"**/*.wav\"):\n",
        "    if not file.is_file(): \n",
        "        continue\n",
        "    audios.append(str(file))\n",
        "\n",
        "# Load and resample audio files\n",
        "audio_list = load_audio_files(audios, resampling_frequency=16000)\n",
        "\n",
        "# Making speakers list and labels list \n",
        "speakers = []\n",
        "labels = []\n",
        "for audio_file in audios:\n",
        "  file_name = audio_file.split('/')[4]\n",
        "  speakers.append(audio_file.split('/')[3])\n",
        "  labels.append(file_name[0])\n",
        "\n",
        "\n",
        "# Verify phase_1\n",
        "print('Number of audio files: {}'.format(len(audio_list)))\n",
        "print('Number of speaker classes: {}'.format(len(set(speakers))))\n",
        "print('Speaker classes: {}'.format(set(speakers)))\n",
        "print('Number of speakers: {}'.format(len(speakers)))\n",
        "print('Number of label classes: {}'.format(len(set(labels))))\n",
        "print('Label classes: {}'.format(set(labels)))\n",
        "print('Number of labels: {}'.format(len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sweIDlX7hWgp"
      },
      "source": [
        "## Getting accuracy of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecCZ0ZWChWgq"
      },
      "outputs": [],
      "source": [
        "# model_names = ['wav2vec', 'hubert']\n",
        "# pipeline(audio_list, speakers, labels, model_names, 'SAVEE')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLAJwd09hWgq",
        "outputId": "8c41d79a-8b6b-4667-97bb-08180daab5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL: hybrid_byols\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings...: 100%|██████████| 480/480 [00:32<00:00, 14.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([480, 2048])\n",
            "normalised_embeddings shape: torch.Size([480, 2048])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([360, 2048])\n",
            "X_test shape: torch.Size([120, 2048])\n",
            "y_train len: 360\n",
            "y_test len: 120\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.8296296296296296\n",
            "Best Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.4888888888888889\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.8481481481481481\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.4388888888888889\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.7351851851851852\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 200}\n",
            "Accuracy on test_set: 0.48888888888888876\n",
            "\n",
            "\n",
            "MODEL: compare\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 480/480 [01:01<00:00,  7.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([480, 6373])\n",
            "normalised_embeddings shape: torch.Size([480, 6373])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([360, 6373])\n",
            "X_test shape: torch.Size([120, 6373])\n",
            "y_train len: 360\n",
            "y_test len: 120\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.6203703703703705\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'sag'}\n",
            "Accuracy on test_set: 0.6166666666666666\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.5907407407407407\n",
            "Best Parameters: {'C': 100.0, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.6111111111111112\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.6148148148148149\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "Accuracy on test_set: 0.5\n",
            "\n",
            "\n",
            "MODEL: egemaps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 480/480 [01:08<00:00,  6.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_array shape: torch.Size([480, 88])\n",
            "normalised_embeddings shape: torch.Size([480, 88])\n",
            "PASSED: All means are less than 10**-6\n",
            "X_train shape: torch.Size([360, 88])\n",
            "X_test shape: torch.Size([120, 88])\n",
            "y_train len: 360\n",
            "y_test len: 120\n",
            "\n",
            "Logistic Regression:\n",
            "Accuracy : 0.65\n",
            "Best Parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Accuracy on test_set: 0.39444444444444443\n",
            "Support Vector Machine:\n",
            "Accuracy : 0.6518518518518518\n",
            "Best Parameters: {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test_set: 0.4777777777777778\n",
            "Random Forest Classifier:\n",
            "Accuracy : 0.7055555555555555\n",
            "Best Parameters: {'bootstrap': False, 'max_features': 'auto', 'n_estimators': 50}\n",
            "Accuracy on test_set: 0.5944444444444444\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ['hybrid_byols', 'compare', 'egemaps']\n",
        "pipeline(audio_list, speakers, labels, model_names, 'SAVEE')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gM3bQimQhq-"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UDwJS5P8QmOk",
        "outputId": "94061827-faee-4411-bb16-7a1508cd127e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 EmoDB      CaFE     ShEMO   CREMA-D   RAVDESS     SAVEE\n",
              "wav2vec       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hubert        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hybrid_byols  0.918284  0.845238  0.540650  0.762123  0.754464  0.488889\n",
              "compare       0.839711  0.654762  0.612687  0.716362  0.662946  0.616667\n",
              "egemaps       0.780539  0.583333  0.502527  0.600993  0.560268  0.394444"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66cc3011-3ec7-47fb-b9a2-f1024709f85d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EmoDB</th>\n",
              "      <th>CaFE</th>\n",
              "      <th>ShEMO</th>\n",
              "      <th>CREMA-D</th>\n",
              "      <th>RAVDESS</th>\n",
              "      <th>SAVEE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>wav2vec</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hubert</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hybrid_byols</th>\n",
              "      <td>0.918284</td>\n",
              "      <td>0.845238</td>\n",
              "      <td>0.540650</td>\n",
              "      <td>0.762123</td>\n",
              "      <td>0.754464</td>\n",
              "      <td>0.488889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compare</th>\n",
              "      <td>0.839711</td>\n",
              "      <td>0.654762</td>\n",
              "      <td>0.612687</td>\n",
              "      <td>0.716362</td>\n",
              "      <td>0.662946</td>\n",
              "      <td>0.616667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>egemaps</th>\n",
              "      <td>0.780539</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.502527</td>\n",
              "      <td>0.600993</td>\n",
              "      <td>0.560268</td>\n",
              "      <td>0.394444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66cc3011-3ec7-47fb-b9a2-f1024709f85d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66cc3011-3ec7-47fb-b9a2-f1024709f85d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66cc3011-3ec7-47fb-b9a2-f1024709f85d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "logistic_regression_results = {'EmoDB': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.9182839802587701, 'compare': 0.8397108471180035, 'egemaps': 0.7805394605394606},\n",
        "        'CaFE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.8452380952380952, 'compare': 0.6547619047619048, 'egemaps': 0.5833333333333334},\n",
        "        'ShEMO': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.5406500133083051, 'compare': 0.6126867592495242, 'egemaps': 0.5025268582186633},\n",
        "        'CREMA-D': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7621230593494462, 'compare': 0.71636159942007, 'egemaps': 0.6009934049181415},\n",
        "        'RAVDESS': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7544642857142857, 'compare': 0.6629464285714286, 'egemaps': 0.5602678571428571},\n",
        "        'SAVEE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.4888888888888889, 'compare': 0.6166666666666666, 'egemaps': 0.39444444444444443}}\n",
        "\n",
        "lr_data = pd.DataFrame(logistic_regression_results)\n",
        "lr_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HZNBwX36QSM-",
        "outputId": "0b90a151-3d9e-4b45-eda0-aeee0676a853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 EmoDB      CaFE     ShEMO   CREMA-D   RAVDESS     SAVEE\n",
              "wav2vec       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hubert        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hybrid_byols  0.891557  0.851190  0.508836  0.727532  0.767857  0.438889\n",
              "compare       0.853026  0.636905  0.614420  0.687204  0.649554  0.611111\n",
              "egemaps       0.771009  0.559524  0.450757  0.608605  0.542411  0.477778"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec261988-61b7-45c8-a844-6d0a397d0e4d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EmoDB</th>\n",
              "      <th>CaFE</th>\n",
              "      <th>ShEMO</th>\n",
              "      <th>CREMA-D</th>\n",
              "      <th>RAVDESS</th>\n",
              "      <th>SAVEE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>wav2vec</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hubert</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hybrid_byols</th>\n",
              "      <td>0.891557</td>\n",
              "      <td>0.851190</td>\n",
              "      <td>0.508836</td>\n",
              "      <td>0.727532</td>\n",
              "      <td>0.767857</td>\n",
              "      <td>0.438889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compare</th>\n",
              "      <td>0.853026</td>\n",
              "      <td>0.636905</td>\n",
              "      <td>0.614420</td>\n",
              "      <td>0.687204</td>\n",
              "      <td>0.649554</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>egemaps</th>\n",
              "      <td>0.771009</td>\n",
              "      <td>0.559524</td>\n",
              "      <td>0.450757</td>\n",
              "      <td>0.608605</td>\n",
              "      <td>0.542411</td>\n",
              "      <td>0.477778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec261988-61b7-45c8-a844-6d0a397d0e4d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec261988-61b7-45c8-a844-6d0a397d0e4d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec261988-61b7-45c8-a844-6d0a397d0e4d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "support_vector_machine_results = {'EmoDB': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.8915566226490597, 'compare': 0.8530255278289977, 'egemaps': 0.7710089910089909},\n",
        "        'CaFE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.8511904761904763, 'compare': 0.6369047619047619, 'egemaps': 0.5595238095238095},\n",
        "        'ShEMO': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.5088360425600119, 'compare': 0.6144201896699778, 'egemaps': 0.45075655053032015},\n",
        "        'CREMA-D': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7275318384763662, 'compare': 0.6872044746857341, 'egemaps': 0.6086048142992864},\n",
        "        'RAVDESS': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7678571428571429, 'compare': 0.6495535714285714, 'egemaps': 0.5424107142857143},\n",
        "        'SAVEE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.4388888888888889, 'compare': 0.6111111111111112, 'egemaps': 0.4777777777777778}}\n",
        "\n",
        "svm_data = pd.DataFrame(support_vector_machine_results)\n",
        "svm_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZkdSC7z8QSPz",
        "outputId": "cb84bc57-7862-4c22-cda2-5ee20069fa13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 EmoDB      CaFE     ShEMO   CREMA-D   RAVDESS     SAVEE\n",
              "wav2vec       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hubert        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "hybrid_byols  0.744783  0.785714  0.383562  0.650511  0.687500  0.488889\n",
              "compare       0.778658  0.607143  0.437932  0.632019  0.582589  0.500000\n",
              "egemaps       0.726607  0.577381  0.430000  0.566419  0.531250  0.594444"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d27d596-095a-4033-a9c8-77eb17fac7d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EmoDB</th>\n",
              "      <th>CaFE</th>\n",
              "      <th>ShEMO</th>\n",
              "      <th>CREMA-D</th>\n",
              "      <th>RAVDESS</th>\n",
              "      <th>SAVEE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>wav2vec</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hubert</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hybrid_byols</th>\n",
              "      <td>0.744783</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.383562</td>\n",
              "      <td>0.650511</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.488889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compare</th>\n",
              "      <td>0.778658</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.437932</td>\n",
              "      <td>0.632019</td>\n",
              "      <td>0.582589</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>egemaps</th>\n",
              "      <td>0.726607</td>\n",
              "      <td>0.577381</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.566419</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.594444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d27d596-095a-4033-a9c8-77eb17fac7d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d27d596-095a-4033-a9c8-77eb17fac7d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d27d596-095a-4033-a9c8-77eb17fac7d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "random_forest_classifier_results = {'EmoDB': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7447827615894843, 'compare': 0.7786581381932427, 'egemaps': 0.7266067266067265},\n",
        "        'CaFE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.7857142857142857, 'compare': 0.6071428571428571, 'egemaps': 0.5773809523809524},\n",
        "        'ShEMO': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.3835617820821429, 'compare': 0.4379319816774401, 'egemaps': 0.4300003123614043},\n",
        "        'CREMA-D': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.6505112828201284, 'compare': 0.632019155257536, 'egemaps': 0.5664189368330351},\n",
        "        'RAVDESS': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.6875, 'compare': 0.5825892857142858, 'egemaps': 0.53125},\n",
        "        'SAVEE': {'wav2vec': 0, 'hubert': 0, 'hybrid_byols': 0.48888888888888876, 'compare': 0.5, 'egemaps': 0.5944444444444444}}\n",
        "\n",
        "rfc_data = pd.DataFrame(random_forest_classifier_results)\n",
        "rfc_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX7gNUs8Y3xx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Datasets_results.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMD84SKkKipGABJmW5D04TX"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
